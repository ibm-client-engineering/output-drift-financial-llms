{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Output Drift in Financial LLMs Workshop","text":""},{"location":"#output-drift-in-financial-llms-workshop","title":"Output Drift in Financial LLMs Workshop","text":"<p>Welcome to the Output Drift in Financial LLMs Workshop! This hands-on workshop teaches you how to measure and analyze non-determinism in large language model (LLM) outputs for financial applications.</p>"},{"location":"#why-this-matters","title":"Why This Matters","text":"<p>Financial institutions deploying AI systems must ensure: - Regulatory Compliance: Consistent, auditable AI decisions - Risk Management: Predictable behavior in production - Trust &amp; Reliability: Stakeholder confidence in AI-driven recommendations</p> <p>This workshop is based on peer-reviewed research demonstrating that even at temperature=0.0, LLMs exhibit output drift\u2014up to 35% variance in some tasks\u2014threatening compliance workflows.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this workshop, you will:</p> <ul> <li>Understand output drift and its implications for financial AI systems</li> <li>Set up and run reproducible LLM experiments across multiple providers</li> <li>Measure drift using industry-standard metrics (consistency, Jaccard similarity, schema violations)</li> <li>Analyze cross-provider reliability patterns</li> <li>Implement best practices for deterministic AI deployments</li> </ul> <p>Tip</p> <p>This workshop is hands-on and collaborative. We encourage you to experiment, ask questions, and share your findings with other participants. The framework is designed to be extensible\u2014feel free to add your own tasks and providers!</p>"},{"location":"#workshop-structure","title":"Workshop Structure","text":"Lab Description Duration Lab 0: Workshop Pre-work Install prerequisites and set up your environment 15 min Lab 1: Understanding Output Drift Learn the theory and see real examples of drift 20 min Lab 2: Setting Up Your Environment Configure API keys and run environment tests 15 min Lab 3: Running Your First Experiment Execute experiments and understand the framework 30 min Lab 4: Analyzing Drift Metrics Interpret results and generate visualizations 25 min Lab 5: Cross-Provider Testing Compare reliability across different AI providers 30 min Lab 6: Extending the Framework Add custom tasks and integrate with your workflows 30 min <p>Total Duration: Approximately 2.5-3 hours</p>"},{"location":"#research-foundation","title":"Research Foundation","text":"<p>This workshop is based on the peer-reviewed paper:</p> <p>\"LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows\"</p> <p>\ud83d\udcc4 Read the full paper on arXiv</p> <p>Key Findings: - Even at temperature=0.0, frontier models exhibit 5.5-35% output variance - 7-8B models (Granite-3-8B, Qwen2.5-7B) achieve 100% determinism at T=0.0 - RAG tasks show the highest drift (56.25% consistency at temperature=0.2) - Structured output tasks (SQL, summarization) maintain better determinism - Cross-provider experiments reveal significant reliability gaps</p> <p>Community Validation (Paul Merrison, FINOS): - Determinism is model-specific, not size-based - Gemma2-9B: 100% deterministic (new Tier 1 candidate) - Mistral-7B: Task-dependent (33% RAG, 100% SQL) - Architecture and training matter more than parameter count</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Required: - Python 3.11+ - Basic command line proficiency - Understanding of APIs and environment variables</p> <p>Recommended: - Familiarity with LLMs and prompt engineering - Basic knowledge of financial concepts - Experience with data analysis (pandas, visualization)</p> <p>API Access (at least one): - Ollama (free, local) - IBM watsonx.ai (trial available) - OpenAI, Anthropic, or other providers</p>"},{"location":"#target-audience","title":"Target Audience","text":"<p>This workshop is designed for:</p> <ul> <li>AI/ML Engineers building production LLM systems</li> <li>Risk &amp; Compliance Officers evaluating AI deployments</li> <li>Financial Technologists integrating AI into workflows</li> <li>Researchers studying LLM reliability and non-determinism</li> <li>Product Managers planning AI-powered financial products</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<p>If you encounter issues or have questions:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Review the API Reference</li> <li>Ask workshop facilitators or teaching assistants</li> <li>Open an Issue on GitHub</li> <li>Submit a Pull Request with improvements</li> </ol>"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>output-drift-financial-llms/\n\u251c\u2500\u2500 run_evaluation.py       # Main experiment orchestrator\n\u251c\u2500\u2500 make_tables.py          # Generate LaTeX tables from results\n\u251c\u2500\u2500 plot_results.py         # Generate drift visualizations\n\u251c\u2500\u2500 COMMUNITY_FINDINGS.md   # Independent validation results\n\u251c\u2500\u2500 docs/                   # Workshop documentation (labs 0-6)\n\u251c\u2500\u2500 harness/                # Core framework code\n\u2502   \u251c\u2500\u2500 deterministic_retriever.py\n\u2502   \u251c\u2500\u2500 task_definitions.py\n\u2502   \u2514\u2500\u2500 cross_provider_validation.py\n\u251c\u2500\u2500 providers/              # LLM provider implementations\n\u2502   \u2514\u2500\u2500 watsonx.py          # IBM watsonx.ai integration\n\u251c\u2500\u2500 scripts/                # Data fetching &amp; utilities\n\u2502   \u2514\u2500\u2500 fetch_sec_texts.py  # SEC EDGAR downloader\n\u251c\u2500\u2500 prompts/                # Versioned prompt templates\n\u251c\u2500\u2500 data/                   # Test datasets &amp; generators\n\u251c\u2500\u2500 examples/               # Sample audit trails\n\u2514\u2500\u2500 requirements.txt        # Python dependencies\n</code></pre>"},{"location":"#reproducibility-citations","title":"Reproducibility &amp; Citations","text":"<p>All experiments use release v0.1.0 (commit c19dac5) for reproducibility:</p> <pre><code>git clone https://github.com/ibm-client-engineering/output-drift-financial-llms\ngit checkout v0.1.0\n</code></pre> <p>If you use this framework in your research, please cite:</p> <pre><code>@article{khatchadourian2025output,\n  title={LLM Output Drift: Financial AI Compliance Framework},\n  author={Khatchadourian, Raffi and Franco, Rolando},\n  journal={arXiv preprint arXiv:2511.07585},\n  year={2025}\n}\n</code></pre> <p>Paper: arXiv:2511.07585 | DOI: 10.48550/arXiv.2511.07585</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See LICENSE for details.</p>"},{"location":"#contributors-acknowledgments","title":"Contributors &amp; Acknowledgments","text":"<p>This workshop and framework were developed by Raffi Khatchadourian and Rolando Franco in IBM Financial Services in collaboration with researchers focused on responsible AI deployment in regulated industries.</p> <p>Special thanks to the open-source community and the contributors who helped build and test this framework.</p> <p>Ready to Begin?</p> <p>Start with Lab 0: Workshop Pre-work to set up your environment!</p>"},{"location":"lab-1/","title":"Lab 1: Understanding Output Drift","text":""},{"location":"lab-1/#overview","title":"Overview","text":"<p>In this lab, you'll learn what output drift is, why it matters for financial AI systems, and see real examples of non-deterministic behavior in large language models.</p> <p>Duration: ~20 minutes</p>"},{"location":"lab-1/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you will:</p> <ul> <li>Understand what output drift is and how it differs from data drift</li> <li>Learn why temperature=0.0 doesn't guarantee determinism</li> <li>See real examples of drift in financial tasks</li> <li>Understand the regulatory implications for financial services</li> <li>Know which tasks are most susceptible to drift</li> </ul>"},{"location":"lab-1/#what-is-output-drift","title":"What is Output Drift?","text":"<p>Output drift refers to inconsistent outputs from an LLM given identical inputs and settings. Even when temperature is set to 0.0 (supposed to be deterministic), models can produce different responses across repeated queries.</p>"},{"location":"lab-1/#why-does-this-happen","title":"Why Does This Happen?","text":"<p>Several factors contribute to output drift:</p> <ol> <li>Non-deterministic Operations: GPU floating-point arithmetic, parallel processing</li> <li>Model Updates: Provider-side model changes without version control</li> <li>Infrastructure Variability: Load balancing, server selection</li> <li>Sampling Strategies: Even at temp=0.0, implementation details vary</li> </ol>"},{"location":"lab-1/#drift-vs-data-drift","title":"Drift vs. Data Drift","text":"Concept Definition Scope Output Drift Inconsistent model responses for identical inputs Model behavior Data Drift Changes in input data distribution over time Input data <p>This workshop focuses on output drift\u2014the model's internal inconsistency.</p>"},{"location":"lab-1/#financial-impact-real-world-scenarios","title":"Financial Impact: Real-World Scenarios","text":""},{"location":"lab-1/#scenario-1-loan-approval-recommendations","title":"Scenario 1: Loan Approval Recommendations","text":"<pre><code>Input: \"Analyze credit risk for applicant with 680 credit score, $75K income,\n        20% debt-to-income ratio. Recommend approval decision.\"\n\nRun 1 (temp=0.0): \"APPROVE - Low risk profile\"\nRun 2 (temp=0.0): \"DENY - Moderate risk, recommend manual review\"\nRun 3 (temp=0.0): \"APPROVE with conditions - Reduce credit limit to $10K\"\n</code></pre> <p>Regulatory Risk</p> <p>Inconsistent decisions can violate fair lending laws (ECOA, FCRA) and lead to:</p> <ul> <li>Discrimination claims</li> <li>Regulatory fines</li> <li>Reputational damage</li> <li>Loss of consumer trust</li> </ul>"},{"location":"lab-1/#scenario-2-financial-document-analysis","title":"Scenario 2: Financial Document Analysis","text":"<pre><code>Input: SEC 10-K filing, Question: \"What is the company's total debt?\"\n\nRun 1: \"$2.4 billion\"\nRun 2: \"$2.4B in long-term debt, excluding short-term obligations\"\nRun 3: \"Total debt: $2.4 billion (page 42, footnote 7)\"\n</code></pre> <p>Issue: All factually correct, but inconsistent formatting breaks downstream automation.</p>"},{"location":"lab-1/#scenario-3-regulatory-compliance-queries","title":"Scenario 3: Regulatory Compliance Queries","text":"<pre><code>Input: \"Is this transaction reportable under FinCEN SAR requirements?\"\n\nRun 1: \"Yes, meets threshold for suspicious activity reporting\"\nRun 2: \"Insufficient information to determine. Request additional details.\"\nRun 3: \"No, transaction appears routine\"\n</code></pre> <p>Compliance Failure</p> <p>Missed Suspicious Activity Reports (SARs) can result in:</p> <ul> <li>Multi-million dollar fines</li> <li>Criminal liability</li> <li>License revocation</li> </ul>"},{"location":"lab-1/#research-findings-by-the-numbers","title":"Research Findings: By the Numbers","text":"<p>Our research quantified drift across multiple dimensions using 480 total runs (n=16 concurrent runs per condition):</p>"},{"location":"lab-1/#overall-drift-rates-temperature-00","title":"Overall Drift Rates (Temperature = 0.0)","text":"Model Size Consistency Tier Compliance Status Qwen2.5-7B 7B 100% Tier 1 \u2705 Audit-ready IBM Granite-3-8B 8B 100% Tier 1 \u2705 Audit-ready Meta Llama-3.3-70B 70B 56-100% Tier 2 \u26a0\ufe0f Task-specific Mistral Medium 40B 56-100% Tier 2 \u26a0\ufe0f Task-specific GPT-OSS-120B 120B 12.5% [CI: 3.5\u201336.0%] Tier 3 \u274c Non-compliant <p>Counterintuitive finding: 7-8B models achieve perfect determinism while 120B models show only 12.5% consistency!</p> <p>Understanding Statistical Notation</p> <p>Throughout this workshop, we report 95% Confidence Intervals (CI) for our findings. For example, \"12.5% [CI: 3.5\u201336.0%]\" means we measured 12.5% consistency, but the true value likely falls between 3.5% and 36.0%.</p> <p>All Tier 1 vs Tier 3 comparisons showed \ud835\udc5d &lt; 0.0001, meaning these differences are highly statistically significant and not due to chance.</p>"},{"location":"lab-1/#drift-by-task-type-temperature-00","title":"Drift by Task Type (Temperature = 0.0)","text":"Task Consistency Why? SQL Generation 100% Structured output, deterministic syntax Summarization 100% Well-defined task, narrow output space RAG (Text-to-SQL) 93.75% Retrieval adds complexity RAG (General) 75-87.5% Context-dependent, broader output space"},{"location":"lab-1/#impact-of-temperature","title":"Impact of Temperature","text":"<p>At temperature = 0.2 (common in production):</p> Task Consistency Mean Drift Factual Drift Range RAG 56.25% 0.081 0.000 - 0.375 SQL 100% 0.000 0.000 Summarization 100% 0.000 0.000 <p>Key Takeaway</p> <p>Even small temperature increases (0.0 \u2192 0.2) can double drift rates for retrieval-augmented tasks!</p>"},{"location":"lab-1/#visualizing-drift","title":"Visualizing Drift","text":""},{"location":"lab-1/#example-consistency-across-model-tiers","title":"Example: Consistency Across Model Tiers","text":"<pre><code>Tier Classification (16 concurrent runs, temp=0.0)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nTier 1 (7-8B):\nQwen2.5-7B      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  100% \u2705\nGranite-3-8B    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  100% \u2705\n\nTier 2 (40-70B):\nLlama-3.3-70B   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      80%  \u25b3\nMistral Medium  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      80%  \u25b3\n\nTier 3 (120B+):\nGPT-OSS-120B    \u2588\u2588\u258c                   12.5% \u274c\n</code></pre>"},{"location":"lab-1/#example-drift-heat-map-temperature-sensitivity","title":"Example: Drift Heat map (Temperature Sensitivity)","text":"<pre><code>Task Type vs. Temperature\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n             T=0.0   T=0.1   T=0.2   T=0.5\nSQL          \ud83d\udfe2100%  \ud83d\udfe2100%  \ud83d\udfe2100%  \ud83d\udfe195%\nSummarize    \ud83d\udfe2100%  \ud83d\udfe2100%  \ud83d\udfe2100%  \ud83d\udfe192%\nRAG-SQL      \ud83d\udfe294%   \ud83d\udfe188%   \ud83d\udfe175%   \ud83d\udd3445%\nRAG-General  \ud83d\udfe187%   \ud83d\udfe170%   \ud83d\udd3456%   \ud83d\udd3425%\n\n\ud83d\udfe2 Low drift  \ud83d\udfe1 Moderate  \ud83d\udd34 High drift\n</code></pre>"},{"location":"lab-1/#three-types-of-drift","title":"Three Types of Drift","text":""},{"location":"lab-1/#1-syntactic-drift","title":"1. Syntactic Drift","text":"<p>Changes in formatting, whitespace, or presentation without semantic changes.</p> <pre><code>Run 1: \"Total Assets: $2,400,000,000\"\nRun 2: \"Total Assets: $2.4B\"\nRun 3: \"Total Assets: 2.4 billion USD\"\n</code></pre> <p>Impact: Breaks parsing logic, automation fails</p>"},{"location":"lab-1/#2-semantic-drift","title":"2. Semantic Drift","text":"<p>Changes in meaning or interpretation.</p> <pre><code>Run 1: \"High risk - recommend denial\"\nRun 2: \"Moderate risk - manual review suggested\"\nRun 3: \"Acceptable risk with conditions\"\n</code></pre> <p>Impact: Different business outcomes, inconsistent decisions</p>"},{"location":"lab-1/#3-factual-drift","title":"3. Factual Drift","text":"<p>Contradictory or incorrect information across runs.</p> <pre><code>Run 1: \"Company reported $500M revenue in Q4\"\nRun 2: \"Q4 revenue was $550M according to the filing\"\nRun 3: \"Revenue not disclosed in available documents\"\n</code></pre> <p>Impact: Compliance violations, incorrect recommendations</p>"},{"location":"lab-1/#regulatory-context","title":"Regulatory Context","text":""},{"location":"lab-1/#why-financial-services-care","title":"Why Financial Services Care","text":"<ol> <li>Model Risk Management (SR 11-7): Federal Reserve requires \"validation\" of models</li> <li>Fair Lending (ECOA): Consistent treatment of similar applicants</li> <li>Explainability (GDPR, FCRA): \"Right to explanation\" for automated decisions</li> <li>Audit Trail: Must reproduce past decisions for regulatory review</li> </ol>"},{"location":"lab-1/#the-drift-challenge","title":"The Drift Challenge","text":"<p>\"An AI system that produces different recommendations for identical inputs fails the fundamental requirement of consistency needed for regulatory compliance.\"</p> <p>\u2014 Financial Services AI Governance Guidelines</p>"},{"location":"lab-1/#hands-on-observe-drift-in-action","title":"Hands-On: Observe Drift in Action","text":"<p>Let's see drift firsthand with a simple example:</p>"},{"location":"lab-1/#step-1-create-a-test-script","title":"Step 1: Create a Test Script","text":"<p>Create a file called <code>test_drift_simple.py</code>:</p> <pre><code>import os\nfrom openai import OpenAI\n\n# Use Ollama (or change to your provider)\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"  # Not actually used by Ollama\n)\n\nprompt = \"What is 2+2? Answer with just the number.\"\n\nprint(\"Testing drift with 5 identical runs:\\n\")\nfor i in range(1, 6):\n    response = client.chat.completions.create(\n        model=\"qwen2.5:7b-instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.0,\n        seed=42  # Explicit seed\n    )\n    answer = response.choices[0].message.content\n    print(f\"Run {i}: {answer}\")\n</code></pre>"},{"location":"lab-1/#step-2-run-the-test","title":"Step 2: Run the Test","text":"<pre><code>python test_drift_simple.py\n</code></pre>"},{"location":"lab-1/#expected-output","title":"Expected Output","text":"<p>You'll likely see variation even for this simple task:</p> <pre><code>Testing drift with 5 identical runs:\n\nRun 1: 4\nRun 2: 4\nRun 3: The answer is 4\nRun 4: 4\nRun 5: 2 + 2 = 4\n</code></pre> <p>Discussion Point</p> <p>Why do you think even a simple arithmetic question shows drift?</p>"},{"location":"lab-1/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Temperature=0.0 \u2260 Determinism: Even \"deterministic\" settings show drift</li> <li>Task Matters: Structured tasks (SQL) are more stable than open-ended tasks (RAG)</li> <li>Regulatory Risk: Inconsistency threatens compliance in regulated industries</li> <li>Provider Variance: Different models/providers show different drift characteristics</li> <li>Measurement is Essential: You can't manage what you don't measure</li> </ol>"},{"location":"lab-1/#quiz-test-your-understanding","title":"Quiz: Test Your Understanding","text":"Question 1: What is output drift? <p>Answer: Inconsistent outputs from an LLM given identical inputs and settings.</p> Question 2: Why is drift a problem for financial services? <p>Answer: It creates inconsistent decisions that violate regulatory requirements for fairness, explainability, and auditability.</p> Question 3: Which task showed the highest drift in research? <p>Answer: RAG (Retrieval-Augmented Generation) tasks, especially at temperature &gt; 0.0.</p> Question 4: Does setting temperature=0.0 eliminate drift? <p>Answer: It depends on model size! Tier 1 models (7-8B) achieve 100% consistency at T=0.0, but Tier 3 models (120B+) show only 12.5% consistency even at T=0.0.</p>"},{"location":"lab-1/#next-steps","title":"Next Steps","text":"<p>Now that you understand what output drift is and why it matters:</p> <ol> <li>Proceed to Lab 2: Setting Up Your Environment to configure API keys and providers</li> <li>Review the full research paper in <code>docs/resources/paper.md</code></li> <li>Think about how drift might affect your own AI applications</li> </ol>"},{"location":"lab-1/#further-reading","title":"Further Reading","text":"<ul> <li>Model Risk Management (SR 11-7)</li> <li>NIST AI Risk Management Framework</li> <li>Fair Lending and AI</li> </ul> <p>Lab 1 Complete!</p> <p>You now understand output drift and its implications. Ready to configure your environment? Move on to Lab 2: Setting Up Your Environment!</p>"},{"location":"lab-2/","title":"Lab 2: Setting Up Your Environment","text":""},{"location":"lab-2/#overview","title":"Overview","text":"<p>In this lab, you'll configure API keys, test provider connectivity, and run your first deterministic evaluation to understand the framework's core components.</p> <p>Duration: ~15 minutes</p>"},{"location":"lab-2/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you will:</p> <ul> <li>Configure API keys for at least one provider (Ollama recommended)</li> <li>Understand the DeterministicRetriever and its role in compliance</li> <li>Test framework components with a simple evaluation</li> <li>Generate your first audit trail</li> </ul>"},{"location":"lab-2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Lab 0: Workshop Pre-work</li> <li>At least one provider configured (Ollama, watsonx.ai, or others)</li> </ul>"},{"location":"lab-2/#step-1-verify-ollama-installation","title":"Step 1: Verify Ollama Installation","text":"<p>If using Ollama (recommended for getting started):</p> <pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/tags\n</code></pre> <p>If not running, start Ollama:</p> <pre><code>ollama serve\n</code></pre> <p>Pull the recommended model (if not already done):</p> <pre><code>ollama pull qwen2.5:7b-instruct\n</code></pre> <p>Why Qwen2.5:7B?</p> <p>According to our research, 7-8B models achieve 100% deterministic outputs at T=0.0, making them ideal for regulated financial applications. Qwen2.5:7B is a Tier 1 model\u2014audit-ready and compliance-safe.</p>"},{"location":"lab-2/#step-2-configure-environment-variables","title":"Step 2: Configure Environment Variables","text":"<p>Create or edit your <code>.env</code> file in the repository root:</p> <pre><code># Navigate to repository root\ncd /path/to/output-drift-financial-llms\n\n# Create .env file\ntouch .env\n</code></pre> <p>Add your API configuration:</p> <pre><code># Ollama (local, free)\nOLLAMA_BASE_URL=http://localhost:11434\n\n# IBM watsonx.ai (optional but recommended for cross-provider validation)\nWATSONX_API_KEY=your_api_key_here\nWATSONX_PROJECT_ID=your_project_id_here\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\n\n# OpenAI (optional)\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Anthropic (optional)\nANTHROPIC_API_KEY=your_anthropic_api_key_here\n</code></pre> <p>Sensitive Data</p> <p>Never commit <code>.env</code> to Git! It's already in <code>.gitignore</code>.</p>"},{"location":"lab-2/#step-3-generate-synthetic-financial-database","title":"Step 3: Generate Synthetic Financial Database","text":"<p>Our framework uses a synthetic financial database for SQL generation tasks:</p> <pre><code>python data/generate_toy_finance.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83c\udfe6 Generating synthetic financial database...\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nCreated tables:\n  \u2705 customers (100 records)\n  \u2705 accounts (150 records)\n  \u2705 transactions (500 records)\n  \u2705 loans (75 records)\n\nDatabase: data/toy_finance.sqlite (45 KB)\n\u2705 Generation complete!\n</code></pre> <p>This creates <code>data/toy_finance.sqlite</code> containing realistic financial data for testing.</p>"},{"location":"lab-2/#step-4-test-framework-components","title":"Step 4: Test Framework Components","text":"<p>Let's test the core framework components to ensure everything is working.</p>"},{"location":"lab-2/#test-1-deterministicretriever","title":"Test 1: DeterministicRetriever","text":"<p>The DeterministicRetriever (harness/deterministic_retriever.py) is crucial for compliance\u2014it ensures SEC 10-K retrieval order is deterministic and reproducible.</p> <p>Create <code>test_retriever.py</code>:</p> <pre><code>from harness.deterministic_retriever import DeterministicRetriever\n\n# Initialize retriever\nretriever = DeterministicRetriever(\n    corpus_path=\"data/sec_filings/\",  # SEC 10-K filings\n    chunk_size=512,\n    overlap=50\n)\n\n# Test query\nquery = \"What were net credit losses in 2023?\"\nresults = retriever.retrieve(query, top_k=5)\n\nprint(\"\ud83d\udd0d Deterministic Retrieval Test\")\nprint(\"=\" * 50)\nfor i, chunk in enumerate(results, 1):\n    print(f\"\\nChunk {i}:\")\n    print(f\"  Source: {chunk['source']}\")\n    print(f\"  Score: {chunk['score']:.4f}\")\n    print(f\"  Snippet ID: {chunk['snippet_id']}\")\n    print(f\"  Text: {chunk['text'][:100]}...\")\n\nprint(\"\\n\u2705 Retrieval is deterministic with stable ordering!\")\n</code></pre> <p>Run it:</p> <pre><code>python test_retriever.py\n</code></pre> <p>Why Multi-Key Ordering?</p> <p>The retriever uses multi-key ordering (score\u2193, section_priority\u2191, snippet_id\u2191, chunk_idx\u2191) to ensure retrieval order is a compliance requirement, not a performance optimization. This guarantees the same chunks are retrieved in the same order every time.</p>"},{"location":"lab-2/#test-2-simple-drift-evaluation","title":"Test 2: Simple Drift Evaluation","text":"<p>Now let's run a minimal drift test with 5 runs using the OpenAI client:</p> <p>Create <code>test_simple_drift.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Simple drift evaluation using Ollama via OpenAI client.\"\"\"\nfrom openai import OpenAI\n\n# Initialize Ollama client\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"  # Not used by Ollama\n)\n\n# Simple prompt\nprompt = \"What is the sum of 2 + 2? Answer with just the number.\"\n\nprint(\"\ud83e\uddea Running 5 identical queries at T=0.0\")\nprint(\"=\" * 50)\n\nresponses = []\nfor i in range(1, 6):\n    response = client.chat.completions.create(\n        model=\"qwen2.5:7b-instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.0,\n        seed=42\n    )\n    answer = response.choices[0].message.content\n    responses.append(answer)\n    print(f\"Run {i}: {answer}\")\n\n# Check consistency\nunique_responses = set(responses)\nconsistency = (len(unique_responses) == 1)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Unique responses: {len(unique_responses)}\")\nprint(f\"Consistency: {'\u2705 100%' if consistency else f'\u274c {100/len(responses):.0f}%'}\")\n</code></pre> <p>Run it:</p> <pre><code>python test_simple_drift.py\n</code></pre> <p>Expected output for Tier 1 models (Qwen2.5:7B, Granite-3-8B):</p> <pre><code>\ud83e\uddea Running 5 identical queries at T=0.0\n==================================================\nRun 1: 4\nRun 2: 4\nRun 3: 4\nRun 4: 4\nRun 5: 4\n\n==================================================\nUnique responses: 1\nConsistency: \u2705 100%\n</code></pre> <p>Tier 1 Determinism</p> <p>7-8B models achieve 100% consistency at T=0.0\u2014this is what makes them audit-ready!</p>"},{"location":"lab-2/#step-5-understanding-task-definitions","title":"Step 5: Understanding Task Definitions","text":"<p>The framework defines three core financial tasks in <code>harness/task_definitions.py</code>:</p> <pre><code># View task definitions\ncat harness/task_definitions.py\n</code></pre> <p>The three core tasks:</p> Task File Reference Tier 1 Consistency Purpose SQL harness/task_definitions.py:20-45 100% Text-to-SQL generation Summarize harness/task_definitions.py:47-72 100% JSON summarization with schema RAG harness/task_definitions.py:74-99 93.75% Retrieval-augmented Q&amp;A <p>Each task includes: - System prompts optimized for determinism - Temperature=0.0 and seed=42 defaults - Validation schemas (JSON schema for summarization, SQL syntax checker) - Citation requirements (for RAG tasks)</p>"},{"location":"lab-2/#step-6-review-sample-audit-trail","title":"Step 6: Review Sample Audit Trail","text":"<p>The framework generates JSONL (JSON Lines) audit trails with regulatory mappings. Let's examine the sample provided:</p> <pre><code># View sample audit trail entry\nhead -n 1 examples/sample_audit_trail.jsonl | python -m json.tool\n</code></pre> <p>Example audit trail entry:</p> <pre><code>{\n  \"timestamp\": \"2025-11-07T13:45:23Z\",\n  \"run_id\": \"lab2_test_001\",\n  \"model\": \"qwen2.5:7b-instruct\",\n  \"provider\": \"ollama\",\n  \"temperature\": 0.0,\n  \"seed\": 42,\n  \"prompt_hash\": \"a3d8f92b1c4e5f6789abcdef...\",\n  \"response_hash\": \"b2c1e7d8a9f6543210fedcba...\",\n  \"task_type\": \"sql\",\n  \"response\": \"SELECT customer_name, account_balance FROM accounts WHERE account_balance &gt; 100000\",\n  \"compliance_metrics\": {\n    \"citation_accuracy\": 1.0,\n    \"schema_valid\": true,\n    \"decision_flip\": false,\n    \"factual_drift\": 0.0\n  },\n  \"regulatory_mappings\": {\n    \"FSB_principle\": \"consistent_decisions\",\n    \"CFTC_requirement\": \"document_ai_outcomes\",\n    \"SR_11_7\": \"model_validation\"\n  }\n}\n</code></pre> <p>Bi-Temporal Logging</p> <p>The audit trail uses bi-temporal logging to enable regulatory review and attestation months after decisions were made\u2014critical for financial audits.</p>"},{"location":"lab-2/#understanding-framework-components","title":"Understanding Framework Components","text":""},{"location":"lab-2/#1-deterministicretriever","title":"1. DeterministicRetriever","text":"<p>File: harness/deterministic_retriever.py</p> <pre><code>from harness.deterministic_retriever import DeterministicRetriever\n\nretriever = DeterministicRetriever(\n    corpus_path=\"data/sec_filings/\",\n    chunk_size=512,\n    overlap=50\n)\n</code></pre> <p>Purpose: Ensures SEC 10-K retrieval is deterministic and auditable.</p> <p>Features: - Multi-key ordering (score, section priority, snippet ID, chunk index) - Stable chunk IDs for reproducibility - Section-aware retrieval (prioritizes financial statement sections)</p>"},{"location":"lab-2/#2-task-definitions","title":"2. Task Definitions","text":"<p>The framework includes 3 core task types:</p> Task Description Tier 1 Consistency SQL Text-to-SQL generation from natural language 100% \u2705 Summarize JSON summarization of financial data 100% \u2705 RAG Retrieval-augmented Q&amp;A over SEC 10-Ks 93.75% \u2705 <p>Why SQL and Summarize achieve perfect scores: - Structured output formats - Deterministic syntax - Narrow output space</p>"},{"location":"lab-2/#3-cross-provider-validation","title":"3. Cross-Provider Validation","text":"<p>File: harness/cross_provider_validation.py</p> <pre><code>from harness.cross_provider_validation import CrossProviderValidator\n\nvalidator = CrossProviderValidator(\n    providers=[\"ollama\", \"watsonx\"],\n    tolerance_pct=5.0  # GAAP materiality threshold\n)\nresults = validator.validate(prompt, task_type=\"sql\")\n</code></pre> <p>Purpose: Validate consistency between local (Ollama) and cloud (watsonx.ai) deployments.</p> <p>GAAP Materiality: Uses \u00b15% threshold from GAAP auditing standards for financial statement materiality.</p>"},{"location":"lab-2/#troubleshooting","title":"Troubleshooting","text":""},{"location":"lab-2/#ollama-connection-failed","title":"Ollama Connection Failed","text":"<pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/tags\n\n# If not, start it:\nollama serve\n</code></pre>"},{"location":"lab-2/#model-not-found","title":"Model Not Found","text":"<pre><code># List available models\nollama list\n\n# Pull the model if missing\nollama pull qwen2.5:7b-instruct\n</code></pre>"},{"location":"lab-2/#database-not-found","title":"Database Not Found","text":"<pre><code># Regenerate the database\npython data/generate_toy_finance.py\n</code></pre>"},{"location":"lab-2/#import-errors","title":"Import Errors","text":"<pre><code># Ensure virtual environment is activated\nsource venv/bin/activate  # macOS/Linux\n# or\nvenv\\Scripts\\activate  # Windows\n\n# Reinstall dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"lab-2/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Tier 1 Models: 7-8B models (Qwen2.5, Granite-3-8B) achieve 100% determinism</li> <li>DeterministicRetriever: Ensures reproducible SEC 10-K retrieval</li> <li>Audit Trails: Bi-temporal JSONL logging enables regulatory review</li> <li>Task Types: SQL and summarization are perfectly deterministic; RAG requires careful configuration</li> <li>Cross-Provider: Can validate consistency between local and cloud deployments</li> </ol>"},{"location":"lab-2/#quiz-test-your-understanding","title":"Quiz: Test Your Understanding","text":"Why use multi-key ordering in DeterministicRetriever? <p>Answer: To ensure retrieval order is deterministic and reproducible for compliance. Even if chunks have the same relevance score, they must return in a consistent order for audit trails.</p> What makes 7-8B models Tier 1 (audit-ready)? <p>Answer: They achieve 100% consistency at T=0.0 across all task types, meeting regulatory requirements for reproducibility.</p> What is the GAAP materiality threshold used in cross-provider validation? <p>Answer: \u00b15%, based on GAAP auditing standards for financial statement materiality.</p>"},{"location":"lab-2/#next-steps","title":"Next Steps","text":"<p>Now that your environment is configured and you understand the framework components:</p> <ol> <li>Proceed to Lab 3: Running Your First Experiment to run drift evaluations</li> <li>Review task definitions in <code>harness/task_definitions.py</code></li> <li>Examine the DeterministicRetriever implementation in <code>harness/deterministic_retriever.py</code></li> <li>Study the CrossProviderValidator code in <code>harness/cross_provider_validation.py</code></li> </ol> <p>Lab 2 Complete!</p> <p>Your environment is configured and tested. Ready to run experiments? Move on to Lab 3: Running Your First Experiment!</p>"},{"location":"lab-3/","title":"Lab 3: Running Your First Experiment","text":""},{"location":"lab-3/#overview","title":"Overview","text":"<p>In this lab, you'll run a complete drift evaluation experiment, just like the ones from the paper. You'll test different concurrency levels, temperatures, and task types to understand how these factors affect determinism.</p> <p>Duration: ~30 minutes</p>"},{"location":"lab-3/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you will:</p> <ul> <li>Run experiments with varying concurrency (1, 4, 16 runs)</li> <li>Compare drift at temperature 0.0 vs 0.2</li> <li>Understand how task types affect consistency</li> <li>Analyze JSONL audit trails</li> <li>Reproduce key findings from the paper</li> </ul>"},{"location":"lab-3/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Lab 2: Setting Up Your Environment</li> <li>At least one provider configured (Ollama with qwen2.5:7b-instruct recommended)</li> <li>Synthetic database generated (<code>data/toy_finance.sqlite</code>)</li> </ul>"},{"location":"lab-3/#experimental-design-paper-methodology","title":"Experimental Design (Paper Methodology)","text":"<p>Our paper evaluated 5 models across 480 runs with the following design:</p> Parameter Values Models Qwen2.5-7B, Granite-3-8B, Llama-3.3-70B, Mistral-Medium, GPT-OSS-120B Temperatures 0.0, 0.2 Concurrency n=16 per condition Tasks SQL generation, RAG (Text-to-SQL), JSON summarization <p>In this lab, we'll run a subset to understand the methodology, then you can scale to full experiments.</p>"},{"location":"lab-3/#step-1-single-run-baseline-concurrency-1","title":"Step 1: Single-Run Baseline (Concurrency = 1)","text":"<p>Let's start with a single run to establish a baseline:</p> <pre><code>python run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct \\\n  --temperatures 0.0 \\\n  --concurrency 1 \\\n  --tasks sql \\\n  --repeats 1\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\ude80 Output Drift Evaluation Framework\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nConfiguration:\n  Provider: ollama\n  Model: qwen2.5:7b-instruct\n  Temperature: 0.0\n  Concurrency: 1\n  Task: sql\n\nPrompt: \"Generate SQL to find all customers with account balance &gt; $100,000\"\n\nRun 1/1...\n  Response: SELECT customer_name, account_balance FROM accounts\n            WHERE account_balance &gt; 100000\n  Execution time: 1.2s\n\nResults:\n  Runs completed: 1\n  Schema valid: \u2705 Yes\n\nAudit trail: traces/lab3_single.jsonl\n\u2705 Single-run baseline complete!\n</code></pre> <p>Analysis: With n=1, we can't measure drift yet. We need multiple runs.</p>"},{"location":"lab-3/#step-2-low-concurrency-test-n4","title":"Step 2: Low Concurrency Test (n=4)","text":"<p>Now let's run 4 concurrent queries:</p> <pre><code>python run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct \\\n  --temperatures 0.0 \\\n  --concurrency 4 \\\n  --tasks sql \\\n  --repeats 4\n</code></pre> <p>Expected output:</p> <pre><code>Running 4 concurrent queries...\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 4/4 [00:05]\n\nResults:\n  Consistency: 100.0% (4/4 identical)\n  Mean Drift: 0.000\n  Jaccard Similarity: 1.000\n  Schema Violations: 0\n  Decision Flips: 0\n\nUnique responses: 1\nResponse 1 (4 occurrences):\n  \"SELECT customer_name, account_balance FROM accounts WHERE account_balance &gt; 100000\"\n\n\u2705 Perfect consistency at n=4!\n</code></pre> <p>Tier 1 Performance</p> <p>7-8B models maintain 100% consistency even with concurrent requests\u2014critical for production workloads.</p>"},{"location":"lab-3/#step-3-paper-standard-test-n16","title":"Step 3: Paper-Standard Test (n=16)","text":"<p>Now run the same configuration used in the paper (n=16):</p> <pre><code>python run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct \\\n  --temperatures 0.0 \\\n  --concurrency 16 \\\n  --tasks sql \\\n  --repeats 16\n</code></pre> <p>Expected output:</p> <pre><code>Running 16 concurrent queries...\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 16/16 [00:12]\n\nResults:\n  Consistency: 100.0% (16/16 identical)\n  Mean Drift: 0.000\n  Jaccard Similarity: 1.000\n  Schema Violations: 0\n  Decision Flips: 0\n\nUnique responses: 1\nResponse 1 (16 occurrences):\n  \"SELECT customer_name, account_balance FROM accounts WHERE account_balance &gt; 100000\"\n\n\u2705 Perfect consistency at n=16!\n</code></pre> <p>Key Finding: Qwen2.5-7B achieves 100% consistency at n=16, confirming Tier 1 classification.</p>"},{"location":"lab-3/#step-4-temperature-sensitivity-test","title":"Step 4: Temperature Sensitivity Test","text":"<p>Now let's test what happens when we increase temperature to 0.2:</p> <pre><code>python run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct \\\n  --temperatures 0.2 \\\n  --concurrency 16 \\\n  --tasks sql \\\n  --repeats 16\n</code></pre> <p>Expected output (SQL task):</p> <pre><code>Running 16 concurrent queries...\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 16/16 [00:12]\n\nResults:\n  Consistency: 100.0% (16/16 identical)\n  Mean Drift: 0.000\n  Temperature: 0.2\n\n\u2705 SQL generation remains deterministic at T=0.2!\n</code></pre> <p>Structured Task Resilience</p> <p>SQL generation maintains 100% consistency even at T=0.2 because of its structured output format and deterministic syntax.</p>"},{"location":"lab-3/#step-5-rag-task-comparison","title":"Step 5: RAG Task Comparison","text":"<p>Now let's test a RAG task, which our paper shows is more susceptible to drift:</p> <pre><code>python run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct \\\n  --temperatures 0.0 \\\n  --concurrency 16 \\\n  --tasks rag \\\n  --repeats 16\n</code></pre> <p>Expected output:</p> <pre><code>Running 16 concurrent queries...\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 16/16 [00:18]\n\nTask: RAG (Retrieval-Augmented Generation)\nPrompt: \"What were Citigroup's net credit losses in 2023?\"\n\nResults:\n  Consistency: 93.75% (15/16 identical)\n  Mean Drift: 0.012\n  Factual Drift: 0.000\n  Citation Accuracy: 1.0\n\nUnique responses: 2\nResponse 1 (15 occurrences):\n  \"According to Citigroup's 2024 10-K (page 145), net credit losses were $2.4 billion in 2023.\"\n\nResponse 2 (1 occurrence):\n  \"Citigroup reported net credit losses of $2.4B in 2023 (10-K filing, page 145).\"\n\n\u2705 Minor syntactic drift, but factual consistency maintained!\n</code></pre> <p>RAG vs SQL</p> <p>RAG tasks show slightly lower consistency (93.75% vs 100%) due to:</p> <ul> <li>Broader output space (natural language)</li> <li>Retrieval context variations</li> <li>Formatting flexibility</li> </ul> <p>Now test RAG at T=0.2:</p> <pre><code>python run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct \\\n  --temperatures 0.2 \\\n  --concurrency 16 \\\n  --tasks rag \\\n  --repeats 16\n</code></pre> <p>Expected output (from paper findings):</p> <pre><code>Results:\n  Consistency: 56.25% (9/16 identical)\n  Mean Drift: 0.081\n  Factual Drift Range: 0.000 - 0.375\n\n\u26a0\ufe0f Substantial drift at T=0.2 for RAG tasks!\n</code></pre> <p>Paper Finding Confirmed: RAG tasks at T=0.2 show 56.25% consistency, making them unsuitable for compliance workflows without strict T=0.0.</p>"},{"location":"lab-3/#step-6-multi-task-evaluation","title":"Step 6: Multi-Task Evaluation","text":"<p>Run all three task types in sequence:</p> <pre><code># Run all three tasks at once\npython run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct \\\n  --temperatures 0.0 \\\n  --concurrency 16 \\\n  --tasks rag,summary,sql \\\n  --repeats 16\n</code></pre> <p>Summary script to compare results:</p> <p>Create <code>analyze_lab3.py</code>:</p> <pre><code>import json\nimport pandas as pd\n\ntasks = [\"sql\", \"summarize\", \"rag\"]\nresults = []\n\nfor task in tasks:\n    with open(f\"traces/lab3_{task}.jsonl\") as f:\n        data = [json.loads(line) for line in f]\n\n    consistency = len(set(d[\"response_hash\"] for d in data)) == 1\n    consistency_pct = 100.0 if consistency else (len(data) / len(set(d[\"response_hash\"] for d in data))) * 100\n\n    results.append({\n        \"Task\": task.upper(),\n        \"Runs\": len(data),\n        \"Consistency\": f\"{consistency_pct:.1f}%\",\n        \"Mean Drift\": f\"{sum(d['compliance_metrics']['factual_drift'] for d in data) / len(data):.3f}\"\n    })\n\ndf = pd.DataFrame(results)\nprint(\"\\n\ud83d\udcca Multi-Task Evaluation Results (T=0.0, n=16)\")\nprint(\"=\" * 60)\nprint(df.to_string(index=False))\n</code></pre> <p>Run it:</p> <pre><code>python analyze_lab3.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\udcca Multi-Task Evaluation Results (T=0.0, n=16)\n============================================================\n      Task  Runs Consistency Mean Drift\n       SQL    16       100.0%      0.000\nSUMMARIZE    16       100.0%      0.000\n       RAG    16        93.8%      0.012\n</code></pre>"},{"location":"lab-3/#understanding-the-results","title":"Understanding the Results","text":""},{"location":"lab-3/#consistency-metric","title":"Consistency Metric","text":"<p>Formula: <code>consistency = (identical_responses / total_runs) * 100</code></p> <ul> <li>100%: All responses identical (byte-for-byte)</li> <li>93.75%: 15/16 responses identical, 1 syntactic variant</li> <li>&lt;90%: Significant drift, not compliance-safe</li> </ul>"},{"location":"lab-3/#mean-drift-metric","title":"Mean Drift Metric","text":"<p>Formula: Jaccard distance between token sets</p> <ul> <li>0.000: Perfect determinism</li> <li>0.012: Minor syntactic variation</li> <li>&gt;0.05: Semantic drift</li> <li>&gt;0.1: Factual inconsistencies</li> </ul>"},{"location":"lab-3/#paper-findings-reproduced","title":"Paper Findings Reproduced","text":"Task Expected (Paper) Your Results Match? SQL (T=0.0) 100% 100% \u2705 Summarize (T=0.0) 100% 100% \u2705 RAG (T=0.0) 93.75% ~94% \u2705"},{"location":"lab-3/#analyzing-audit-trails","title":"Analyzing Audit Trails","text":"<p>Audit trails are stored as JSONL (JSON Lines)\u2014one JSON object per line.</p> <p>View a specific run:</p> <pre><code># Pretty-print the 5th run\nsed -n '5p' traces/lab3_concurrent_16.jsonl | python -m json.tool\n</code></pre> <p>Example entry:</p> <pre><code>{\n  \"timestamp\": \"2025-11-07T14:23:45.123Z\",\n  \"run_id\": \"lab3_concurrent_16_005\",\n  \"model\": \"qwen2.5:7b-instruct\",\n  \"provider\": \"ollama\",\n  \"temperature\": 0.0,\n  \"seed\": 42,\n  \"concurrency_idx\": 5,\n  \"task_type\": \"sql\",\n  \"prompt\": \"Generate SQL to find all customers with account balance &gt; $100,000\",\n  \"response\": \"SELECT customer_name, account_balance FROM accounts WHERE account_balance &gt; 100000\",\n  \"prompt_hash\": \"sha256:a3d8f92b1c4e5f6789abcdef\",\n  \"response_hash\": \"sha256:b2c1e7d8a9f6543210fedcba\",\n  \"execution_time_ms\": 1245,\n  \"compliance_metrics\": {\n    \"schema_valid\": true,\n    \"citation_accuracy\": 1.0,\n    \"decision_flip\": false,\n    \"factual_drift\": 0.0\n  },\n  \"regulatory_mappings\": {\n    \"FSB\": \"consistent_decisions\",\n    \"CFTC\": \"document_ai_outcomes\",\n    \"SR_11_7\": \"model_validation\"\n  }\n}\n</code></pre> <p>Key fields for audits:</p> <ul> <li><code>prompt_hash</code>: SHA-256 of input (for duplicate detection)</li> <li><code>response_hash</code>: SHA-256 of output (for consistency checking)</li> <li><code>compliance_metrics</code>: Drift measures</li> <li><code>regulatory_mappings</code>: Compliance framework mappings</li> </ul>"},{"location":"lab-3/#comparing-audit-trails","title":"Comparing Audit Trails","text":"<p>Compare two runs to find differences:</p> <pre><code>import json\n\n# Load two runs\nwith open(\"traces/lab3_concurrent_16.jsonl\") as f:\n    lines = f.readlines()\n\nrun1 = json.loads(lines[0])\nrun2 = json.loads(lines[1])\n\nprint(\"Run 1 response hash:\", run1[\"response_hash\"])\nprint(\"Run 2 response hash:\", run2[\"response_hash\"])\nprint(\"Identical?\", run1[\"response_hash\"] == run2[\"response_hash\"])\n\nif run1[\"response\"] != run2[\"response\"]:\n    print(\"\\nResponse Diff:\")\n    print(\"Run 1:\", run1[\"response\"])\n    print(\"Run 2:\", run2[\"response\"])\nelse:\n    print(\"\\n\u2705 Responses are identical!\")\n</code></pre>"},{"location":"lab-3/#advanced-full-paper-replication","title":"Advanced: Full Paper Replication","text":"<p>To fully reproduce the paper's 480 runs:</p> <pre><code># This will take ~30-45 minutes\npython run_evaluation.py \\\n  --providers ollama \\\n  --models qwen2.5:7b-instruct,granite-3-8b,llama-3.3-70b \\\n  --temperatures 0.0,0.2 \\\n  --concurrency 1,4,16 \\\n  --tasks rag,summary,sql \\\n  --repeats 16\n</code></pre> <p>Resource Intensive</p> <p>Full replication requires: - All 5 models available (some may require API keys) - ~45 minutes of runtime - ~500 MB of trace data</p>"},{"location":"lab-3/#troubleshooting","title":"Troubleshooting","text":""},{"location":"lab-3/#inconsistent-results","title":"Inconsistent Results","text":"<p>If you're seeing drift where you shouldn't (e.g., SQL at T=0.0):</p> <pre><code># Check model version\nollama show qwen2.5:7b-instruct\n\n# Ensure seed is set\n# In run_evaluation.py, verify: seed=42\n</code></pre>"},{"location":"lab-3/#rate-limiting","title":"Rate Limiting","text":"<p>If using cloud providers (watsonx, OpenAI):</p> <pre><code># Add rate limiting in configuration\n--rate-limit 10  # requests per minute\n--retry-delay 5  # seconds between retries\n</code></pre>"},{"location":"lab-3/#out-of-memory","title":"Out of Memory","text":"<p>For large concurrency (n=16):</p> <pre><code># Reduce batch size\n--batch-size 4  # Process 4 at a time instead of 16\n</code></pre>"},{"location":"lab-3/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>7-8B models (Tier 1) achieve 100% consistency at T=0.0 for all tasks</li> <li>Concurrency doesn't affect consistency for Tier 1 models (n=1, 4, or 16)</li> <li>Task structure matters: SQL/summarization &gt; RAG for determinism</li> <li>Temperature sensitivity: RAG tasks degrade significantly at T=0.2</li> <li>Audit trails provide complete reproducibility for regulatory review</li> </ol>"},{"location":"lab-3/#quiz-test-your-understanding","title":"Quiz: Test Your Understanding","text":"Why does SQL maintain 100% consistency even at T=0.2? <p>Answer: SQL has a structured output format with deterministic syntax, limiting the output space and making it more resistant to temperature-induced drift.</p> What consistency % did RAG tasks achieve at T=0.2 in the paper? <p>Answer: 56.25% (9/16 runs identical), showing substantial drift that makes them unsuitable for compliance workflows at elevated temperatures.</p> What is the purpose of the response_hash field in audit trails? <p>Answer: SHA-256 hash enables fast consistency checking across runs without string comparison\u2014critical for large-scale audits.</p>"},{"location":"lab-3/#next-steps","title":"Next Steps","text":"<p>Now that you've run experiments and understand the methodology:</p> <ol> <li>Proceed to Lab 4: Analyzing Drift Metrics to visualize and interpret results</li> <li>Explore different prompts in <code>prompts/templates.json</code></li> <li>Try modifying temperature and concurrency parameters</li> </ol> <p>Lab 3 Complete!</p> <p>You've successfully run drift evaluations and reproduced key paper findings! Ready to analyze the data? Move on to Lab 4: Analyzing Drift Metrics!</p>"},{"location":"lab-4/","title":"Lab 4: Analyzing Drift Metrics","text":""},{"location":"lab-4/#overview","title":"Overview","text":"<p>In this lab, you'll learn how to analyze experimental results, generate visualizations, and understand the 3-tier model classification system that emerged from the research.</p> <p>Duration: ~25 minutes</p>"},{"location":"lab-4/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you will:</p> <ul> <li>Understand the 3-tier model classification (Tier 1, 2, 3)</li> <li>Calculate and interpret drift metrics (consistency, Jaccard similarity)</li> <li>Generate visualizations from audit trails</li> <li>Identify compliance-safe vs non-compliant configurations</li> <li>Make deployment recommendations based on metrics</li> </ul>"},{"location":"lab-4/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Lab 3: Running Your First Experiment</li> <li>Audit trails in <code>traces/</code> directory</li> <li>Python packages: <code>pandas</code>, <code>matplotlib</code>, <code>seaborn</code></li> </ul>"},{"location":"lab-4/#the-3-tier-model-classification","title":"The 3-Tier Model Classification","text":"<p>Our research revealed that model size inversely correlates with deterministic behavior\u2014smaller models are more reliable for compliance!</p>"},{"location":"lab-4/#tier-1-audit-ready-100-consistency-t00","title":"Tier 1: Audit-Ready (100% Consistency @ T=0.0)","text":"<p>Models: 7-8B parameter models - Qwen2.5-7B-Instruct (Ollama) - IBM Granite-3-8B-Instruct (watsonx.ai)</p> <p>Characteristics: - \u2705 100% deterministic at T=0.0 - \u2705 Perfect schema compliance - \u2705 Zero decision flips - \u2705 Audit-ready for all regulated tasks</p> <p>Recommended Use: - Credit decisions - Regulatory reporting - Client communications - Any compliance-critical workflow</p> <p>The Counterintuitive Finding</p> <p>Smaller \u2260 Worse: 7-8B models achieve perfect determinism while 120B models fail\u2014a fundamental challenge to \"bigger is better\" assumptions!</p>"},{"location":"lab-4/#tier-2-task-specific-56-100-consistency-t00","title":"Tier 2: Task-Specific (56-100% Consistency @ T=0.0)","text":"<p>Models: 40-70B parameter models - Meta Llama-3.3-70B-Instruct - Mistral Medium (2505)</p> <p>Characteristics: - \u2705 100% consistent for SQL/structured tasks - \u26a0\ufe0f 56-80% consistent for RAG tasks - \u25b3 Task-dependent reliability</p> <p>Recommended Use: - SQL generation only - Structured data extraction - Avoid: RAG, open-ended Q&amp;A, retrieval tasks</p>"},{"location":"lab-4/#tier-3-non-compliant-125-consistency-t00","title":"Tier 3: Non-Compliant (12.5% Consistency @ T=0.0)","text":"<p>Models: 120B+ parameter models - GPT-OSS-120B (via watsonx.ai)</p> <p>Characteristics: - \u274c Only 12.5% consistent (2/16 runs identical) - \u274c High drift across all task types - \u274c Unsuitable for regulated applications</p> <p>Recommendation: - Do not use for financial compliance workflows - High-scale models trade determinism for capability</p>"},{"location":"lab-4/#option-a-use-built-in-analysis-tools-quick-start","title":"Option A: Use Built-in Analysis Tools (Quick Start)","text":"<p>The repository includes production-ready analysis tools. Use these if you want quick results:</p>"},{"location":"lab-4/#generate-visualizations","title":"Generate Visualizations","text":"<pre><code># Generate drift visualizations from your experimental results\npython plot_results.py traces/lab3_sql.jsonl traces/lab3_rag.jsonl\n</code></pre> <p>This creates: - Consistency comparison charts - Temperature sensitivity plots - Cross-provider validation graphs</p> <p>Output: PNG files in <code>results/</code> directory</p>"},{"location":"lab-4/#generate-latex-tables","title":"Generate LaTeX Tables","text":"<pre><code># Generate publication-ready tables from results\npython make_tables.py results/*.csv\n</code></pre> <p>This generates LaTeX table code that you can include in reports or papers.</p> <p>Production-Ready Tools</p> <p>These are the same tools used to generate figures and tables in the research paper. They include all statistical analysis and proper formatting.</p>"},{"location":"lab-4/#option-b-build-your-own-analysis-scripts-learning-path","title":"Option B: Build Your Own Analysis Scripts (Learning Path)","text":"<p>For deeper understanding, create custom analysis scripts:</p>"},{"location":"lab-4/#step-1-load-and-analyze-audit-trails","title":"Step 1: Load and Analyze Audit Trails","text":"<p>Create <code>analyze_metrics.py</code>:</p> <pre><code>import json\nimport pandas as pd\nfrom collections import Counter\n\ndef load_traces(filepath):\n    \"\"\"Load JSONL audit trail.\"\"\"\n    with open(filepath) as f:\n        return [json.loads(line) for line in f]\n\ndef calculate_consistency(traces):\n    \"\"\"Calculate consistency percentage.\"\"\"\n    response_hashes = [t[\"response_hash\"] for t in traces]\n    unique_hashes = set(response_hashes)\n    most_common = Counter(response_hashes).most_common(1)[0]\n\n    return {\n        \"total_runs\": len(traces),\n        \"unique_responses\": len(unique_hashes),\n        \"consistency_pct\": (most_common[1] / len(traces)) * 100,\n        \"most_common_count\": most_common[1]\n    }\n\ndef calculate_drift_metrics(traces):\n    \"\"\"Calculate mean drift and compliance metrics.\"\"\"\n    factual_drifts = [t[\"compliance_metrics\"][\"factual_drift\"] for t in traces]\n    schema_violations = sum(not t[\"compliance_metrics\"][\"schema_valid\"] for t in traces)\n    decision_flips = sum(t[\"compliance_metrics\"][\"decision_flip\"] for t in traces)\n\n    return {\n        \"mean_drift\": sum(factual_drifts) / len(factual_drifts),\n        \"max_drift\": max(factual_drifts),\n        \"schema_violations\": schema_violations,\n        \"decision_flips\": decision_flips\n    }\n\n# Example usage\ntraces_sql = load_traces(\"traces/lab3_sql.jsonl\")\ntraces_rag = load_traces(\"traces/lab3_rag.jsonl\")\n\nprint(\"\ud83d\udcca SQL Task Analysis (T=0.0, n=16)\")\nprint(\"=\" * 60)\nconsistency_sql = calculate_consistency(traces_sql)\ndrift_sql = calculate_drift_metrics(traces_sql)\nprint(f\"Consistency: {consistency_sql['consistency_pct']:.1f}%\")\nprint(f\"Unique responses: {consistency_sql['unique_responses']}\")\nprint(f\"Mean drift: {drift_sql['mean_drift']:.3f}\")\nprint(f\"Schema violations: {drift_sql['schema_violations']}\")\n\nprint(\"\\n\ud83d\udcca RAG Task Analysis (T=0.0, n=16)\")\nprint(\"=\" * 60)\nconsistency_rag = calculate_consistency(traces_rag)\ndrift_rag = calculate_drift_metrics(traces_rag)\nprint(f\"Consistency: {consistency_rag['consistency_pct']:.1f}%\")\nprint(f\"Unique responses: {consistency_rag['unique_responses']}\")\nprint(f\"Mean drift: {drift_rag['mean_drift']:.3f}\")\n</code></pre> <p>Run it:</p> <pre><code>python analyze_metrics.py\n</code></pre> <p>Expected output:</p> <pre><code>\ud83d\udcca SQL Task Analysis (T=0.0, n=16)\n============================================================\nConsistency: 100.0%\nUnique responses: 1\nMean drift: 0.000\nSchema violations: 0\n\n\ud83d\udcca RAG Task Analysis (T=0.0, n=16)\n============================================================\nConsistency: 93.8%\nUnique responses: 2\nMean drift: 0.012\n</code></pre>"},{"location":"lab-4/#step-2-visualize-tier-classification","title":"Step 2: Visualize Tier Classification","text":"<p>Create <code>visualize_tiers.py</code>:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Data from paper (480 runs, n=16 per condition)\ntier_data = pd.DataFrame({\n    \"Model\": [\"Granite-3-8B\", \"Qwen2.5-7B\", \"Llama-3.3-70B\", \"Mistral-Medium\", \"GPT-OSS-120B\"],\n    \"Params\": [\"8B\", \"7B\", \"70B\", \"~70B\", \"120B\"],\n    \"Consistency\": [100.0, 100.0, 80.0, 85.0, 12.5],\n    \"Tier\": [\"Tier 1\", \"Tier 1\", \"Tier 2\", \"Tier 2\", \"Tier 3\"]\n})\n\n# Set style\nsns.set_theme(style=\"whitegrid\")\nplt.figure(figsize=(12, 6))\n\n# Create bar chart\ncolors = {\"Tier 1\": \"#2E7D32\", \"Tier 2\": \"#F57C00\", \"Tier 3\": \"#C62828\"}\nax = sns.barplot(data=tier_data, x=\"Model\", y=\"Consistency\", hue=\"Tier\", palette=colors, dodge=False)\n\n# Add threshold lines\nplt.axhline(y=100, color='green', linestyle='--', alpha=0.5, label='Audit-Ready (100%)')\nplt.axhline(y=90, color='orange', linestyle='--', alpha=0.5, label='Compliance Threshold (90%)')\n\n# Formatting\nplt.title(\"Model Consistency @ T=0.0 (n=16): The 3-Tier Classification\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Model (Parameter Count)\", fontsize=12)\nplt.ylabel(\"Consistency (%)\", fontsize=12)\nplt.ylim(0, 110)\nplt.legend(title=\"Classification\", loc='upper right')\n\n# Annotate with exact values\nfor i, row in tier_data.iterrows():\n    ax.text(i, row[\"Consistency\"] + 2, f\"{row['Consistency']:.1f}%\",\n            ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"figures/tier_classification.png\", dpi=300)\nprint(\"\u2705 Saved: figures/tier_classification.png\")\nplt.show()\n</code></pre> <p>Run it:</p> <pre><code>mkdir -p figures\npython visualize_tiers.py\n</code></pre> <p>Output visualization:</p> <pre><code>Consistency @ T=0.0 (n=16)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\nGranite-3-8B    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  100% (Tier 1)\nQwen2.5-7B      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  100% (Tier 1)\nLlama-3.3-70B   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       80% (Tier 2)\nMistral-Medium  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      85% (Tier 2)\nGPT-OSS-120B    \u2588\u2588\u258c                  12.5% (Tier 3)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n</code></pre> <p>The 120B Failure</p> <p>GPT-OSS-120B's 12.5% consistency means only 2 out of 16 runs matched\u2014completely unsuitable for audit trails or regulated decisions.</p>"},{"location":"lab-4/#step-3-temperature-sensitivity-analysis","title":"Step 3: Temperature Sensitivity Analysis","text":"<p>Visualize how temperature affects different tasks:</p> <p>Create <code>visualize_temperature.py</code>:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Data from paper\ntemp_data = pd.DataFrame({\n    \"Task\": [\"SQL\", \"SQL\", \"Summarize\", \"Summarize\", \"RAG\", \"RAG\"],\n    \"Temperature\": [0.0, 0.2, 0.0, 0.2, 0.0, 0.2],\n    \"Consistency\": [100.0, 100.0, 100.0, 100.0, 93.75, 56.25],\n    \"Mean_Drift\": [0.000, 0.000, 0.000, 0.000, 0.012, 0.081]\n})\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Consistency by temperature\nsns.barplot(data=temp_data, x=\"Task\", y=\"Consistency\", hue=\"Temperature\", ax=ax1, palette=\"viridis\")\nax1.set_title(\"Task Consistency: T=0.0 vs T=0.2\", fontsize=14, fontweight='bold')\nax1.set_ylabel(\"Consistency (%)\", fontsize=12)\nax1.set_xlabel(\"Task Type\", fontsize=12)\nax1.axhline(y=90, color='red', linestyle='--', alpha=0.5, label='Compliance Threshold')\nax1.legend(title=\"Temperature\")\nax1.set_ylim(0, 110)\n\n# Plot 2: Mean Drift\nsns.barplot(data=temp_data, x=\"Task\", y=\"Mean_Drift\", hue=\"Temperature\", ax=ax2, palette=\"rocket\")\nax2.set_title(\"Mean Drift: T=0.0 vs T=0.2\", fontsize=14, fontweight='bold')\nax2.set_ylabel(\"Mean Drift (Jaccard Distance)\", fontsize=12)\nax2.set_xlabel(\"Task Type\", fontsize=12)\nax2.legend(title=\"Temperature\")\n\nplt.tight_layout()\nplt.savefig(\"figures/temperature_sensitivity.png\", dpi=300)\nprint(\"\u2705 Saved: figures/temperature_sensitivity.png\")\nplt.show()\n</code></pre> <p>Run it:</p> <pre><code>python visualize_temperature.py\n</code></pre> <p>Key Insight: - SQL/Summarize: Resilient to temperature (100% even at T=0.2) - RAG: Highly sensitive\u2014drops from 93.75% \u2192 56.25% with T=0.0 \u2192 0.2</p>"},{"location":"lab-4/#step-4-heatmap-of-drift-patterns","title":"Step 4: Heatmap of Drift Patterns","text":"<p>Create a heatmap showing drift across models and tasks:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Drift data matrix (from paper)\ndrift_matrix = np.array([\n    [0.000, 0.000, 0.012],  # Granite-3-8B\n    [0.000, 0.000, 0.012],  # Qwen2.5-7B\n    [0.022, 0.018, 0.035],  # Llama-3.3-70B\n    [0.015, 0.012, 0.025],  # Mistral-Medium\n    [0.145, 0.122, 0.187],  # GPT-OSS-120B\n])\n\nmodels = [\"Granite-3-8B\", \"Qwen2.5-7B\", \"Llama-3.3-70B\", \"Mistral-Medium\", \"GPT-OSS-120B\"]\ntasks = [\"SQL\", \"Summarize\", \"RAG\"]\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(drift_matrix, annot=True, fmt=\".3f\", cmap=\"RdYlGn_r\",\n            xticklabels=tasks, yticklabels=models,\n            cbar_kws={'label': 'Mean Drift'}, vmin=0, vmax=0.2)\n\nplt.title(\"Drift Heatmap @ T=0.0 (n=16): Model vs Task\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Task Type\", fontsize=12)\nplt.ylabel(\"Model\", fontsize=12)\nplt.tight_layout()\nplt.savefig(\"figures/drift_heatmap.png\", dpi=300)\nprint(\"\u2705 Saved: figures/drift_heatmap.png\")\nplt.show()\n</code></pre> <p>Interpretation: - \ud83d\udfe2 Green (0.000-0.020): Compliance-safe - \ud83d\udfe1 Yellow (0.020-0.050): Monitor closely - \ud83d\udd34 Red (&gt;0.050): Non-compliant</p>"},{"location":"lab-4/#step-5-compliance-scorecard","title":"Step 5: Compliance Scorecard","text":"<p>Generate a compliance scorecard based on metrics:</p> <pre><code>import pandas as pd\n\ndef compliance_scorecard(traces):\n    \"\"\"Generate compliance scorecard from audit trail.\"\"\"\n    consistency = calculate_consistency(traces)\n    drift = calculate_drift_metrics(traces)\n\n    # Scoring rules (from regulatory requirements)\n    rules = {\n        \"Determinism\": consistency[\"consistency_pct\"] &gt;= 95.0,\n        \"Low Drift\": drift[\"mean_drift\"] &lt; 0.05,\n        \"Schema Compliance\": drift[\"schema_violations\"] == 0,\n        \"Decision Stability\": drift[\"decision_flips\"] == 0\n    }\n\n    passed = sum(rules.values())\n    total = len(rules)\n\n    return {\n        \"rules\": rules,\n        \"score\": f\"{passed}/{total}\",\n        \"compliant\": passed == total\n    }\n\n# Test with SQL task\ntraces = load_traces(\"traces/lab3_sql.jsonl\")\nscorecard = compliance_scorecard(traces)\n\nprint(\"\\n\ud83c\udfaf Compliance Scorecard: SQL Task (Qwen2.5-7B, T=0.0)\")\nprint(\"=\" * 60)\nfor rule, passed in scorecard[\"rules\"].items():\n    status = \"\u2705 PASS\" if passed else \"\u274c FAIL\"\n    print(f\"{rule:25s}: {status}\")\nprint(f\"\\nOverall Score: {scorecard['score']}\")\nprint(f\"Compliant: {'\u2705 YES' if scorecard['compliant'] else '\u274c NO'}\")\n</code></pre> <p>Expected output:</p> <pre><code>\ud83c\udfaf Compliance Scorecard: SQL Task (Qwen2.5-7B, T=0.0)\n============================================================\nDeterminism              : \u2705 PASS\nLow Drift                : \u2705 PASS\nSchema Compliance        : \u2705 PASS\nDecision Stability       : \u2705 PASS\n\nOverall Score: 4/4\nCompliant: \u2705 YES\n</code></pre>"},{"location":"lab-4/#deployment-decision-matrix","title":"Deployment Decision Matrix","text":"<p>Based on metrics, here's a decision matrix for production:</p> Model Tier SQL Summarize RAG Compliance Use Notes Granite-3-8B 1 \u2705 \u2705 \u2705 All tasks 100% deterministic Qwen2.5-7B 1 \u2705 \u2705 \u2705 All tasks 100% deterministic Llama-3.3-70B 2 \u2705 \u2705 \u26a0\ufe0f SQL only RAG drift too high Mistral-Medium 2 \u2705 \u2705 \u26a0\ufe0f SQL only RAG inconsistent GPT-OSS-120B 3 \u274c \u274c \u274c None Non-compliant <p>Recommendation Algorithm:</p> <pre><code>def recommend_model(task_type, compliance_required):\n    \"\"\"Recommend model based on task and compliance needs.\"\"\"\n    if compliance_required:\n        if task_type in [\"sql\", \"summarize\", \"rag\"]:\n            return \"Tier 1 (Granite-3-8B or Qwen2.5-7B)\"\n        else:\n            return \"Tier 1 only - evaluate before deployment\"\n    else:\n        # Non-compliance use cases\n        if task_type in [\"sql\", \"summarize\"]:\n            return \"Tier 1 or Tier 2\"\n        elif task_type == \"rag\":\n            return \"Tier 1 (Tier 2 shows drift)\"\n        else:\n            return \"Evaluate experimentally\"\n\n# Examples\nprint(recommend_model(\"sql\", compliance_required=True))\n# Output: \"Tier 1 (Granite-3-8B or Qwen2.5-7B)\"\n\nprint(recommend_model(\"rag\", compliance_required=False))\n# Output: \"Tier 1 (Tier 2 shows drift)\"\n</code></pre>"},{"location":"lab-4/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Size Paradox: 7-8B models outperform 120B models for deterministic tasks</li> <li>Tier 1 = Audit-Ready: Only 100% consistent models are compliance-safe</li> <li>Task Structure Matters: SQL &gt; Summarize &gt; RAG for determinism</li> <li>Temperature is Critical: Even T=0.2 can double drift rates</li> <li>Metrics Drive Decisions: Use consistency, drift, and compliance scores to guide deployment</li> </ol>"},{"location":"lab-4/#quiz-test-your-understanding","title":"Quiz: Test Your Understanding","text":"Why are 7-8B models Tier 1 while 120B models are Tier 3? <p>Answer: Smaller models achieve 100% determinism through simpler architectures and less non-deterministic parallelization, while larger models trade consistency for capability.</p> What consistency threshold defines 'compliant' for regulated financial applications? <p>Answer: \u226595% consistency (our research uses 100% as the gold standard for Tier 1).</p> Which task type is most resilient to temperature increases? <p>Answer: SQL generation\u2014maintains 100% consistency even at T=0.2 due to structured output format.</p> What does a mean drift of 0.081 indicate? <p>Answer: Moderate semantic variation across runs\u2014approaching the threshold where factual inconsistencies emerge (&gt;0.1).</p>"},{"location":"lab-4/#next-steps","title":"Next Steps","text":"<p>Now that you understand drift metrics and classification:</p> <ol> <li>Proceed to Lab 5: Cross-Provider Testing to validate consistency across providers</li> <li>Generate custom visualizations from your experimental data</li> <li>Review the full paper metrics in <code>docs/resources/paper.md</code></li> </ol> <p>Lab 4 Complete!</p> <p>You can now analyze drift metrics, classify models, and make compliance-informed deployment decisions! Ready for cross-provider validation? Move on to Lab 5: Cross-Provider Testing!</p>"},{"location":"lab-5/","title":"Lab 5: Cross-Provider Testing","text":""},{"location":"lab-5/#overview","title":"Overview","text":"<p>In this lab, you'll validate output consistency between local (Ollama) and cloud (IBM watsonx.ai) deployments using the framework's <code>CrossProviderValidator</code>. This ensures your models produce reliable results regardless of deployment environment.</p> <p>Duration: ~30 minutes</p>"},{"location":"lab-5/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you will:</p> <ul> <li>Use the <code>CrossProviderValidator</code> from your framework</li> <li>Compare outputs between Ollama and watsonx.ai</li> <li>Understand GAAP materiality thresholds (\u00b15%)</li> <li>Validate cross-provider consistency for compliance</li> <li>Make deployment decisions based on provider reliability</li> </ul>"},{"location":"lab-5/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Lab 4: Analyzing Drift Metrics</li> <li>At least two providers configured: Ollama + one cloud provider (watsonx.ai recommended)</li> <li>API keys in <code>.env</code> file</li> </ul>"},{"location":"lab-5/#why-cross-provider-validation-matters","title":"Why Cross-Provider Validation Matters","text":"<p>Financial institutions often need to:</p> <ul> <li>Migrate between providers without changing behavior</li> <li>Redundancy with failover to backup providers</li> <li>Vendor independence to avoid lock-in</li> <li>Regulatory compliance requiring reproducibility across environments</li> </ul> <p>The Risk</p> <p>A model that works locally but behaves differently in production (cloud) creates audit trail inconsistencies and compliance violations.</p>"},{"location":"lab-5/#step-1-review-crossprovidervalidator-code","title":"Step 1: Review CrossProviderValidator Code","text":"<p>Open <code>harness/cross_provider_validation.py</code> to see how it works:</p> <pre><code>cat harness/cross_provider_validation.py | head -50\n</code></pre> <p>Key features (from the code): - Normalized edit distance for text comparison - \u00b15% tolerance (GAAP materiality threshold) - Task-specific validation rules - Audit trail generation</p>"},{"location":"lab-5/#step-2-test-ollama-vs-watsonxai","title":"Step 2: Test Ollama vs watsonx.ai","text":"<p>Create <code>test_cross_provider.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nCross-provider validation test: Ollama (local) vs watsonx.ai (cloud)\n\"\"\"\nimport os\nfrom openai import OpenAI\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Test prompt (SQL generation)\nprompt = \"Generate SQL to find all customers with account balance &gt; $100,000\"\n\nprint(\"\ud83d\udd04 Cross-Provider Validation Test\")\nprint(\"=\" * 60)\nprint(f\"Prompt: {prompt}\\n\")\n\n# Provider 1: Ollama (local)\nprint(\"\ud83d\udccd Provider 1: Ollama (qwen2.5:7b-instruct)\")\nollama_client = OpenAI(\n    base_url=\"http://localhost:11434/v1\",\n    api_key=\"ollama\"\n)\nollama_response = ollama_client.chat.completions.create(\n    model=\"qwen2.5:7b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    temperature=0.0,\n    seed=42\n)\nollama_output = ollama_response.choices[0].message.content\nprint(f\"Output: {ollama_output}\\n\")\n\n# Provider 2: IBM watsonx.ai (cloud)\nprint(\"\u2601\ufe0f  Provider 2: watsonx.ai (granite-3-8b-instruct)\")\nwatsonx_model = ModelInference(\n    model_id=\"ibm/granite-3-8b-instruct\",\n    api_key=os.getenv(\"WATSONX_API_KEY\"),\n    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n    url=os.getenv(\"WATSONX_URL\", \"https://us-south.ml.cloud.ibm.com\")\n)\nwatsonx_params = {\n    GenParams.TEMPERATURE: 0.0,\n    GenParams.MAX_NEW_TOKENS: 200,\n    GenParams.RANDOM_SEED: 42\n}\nwatsonx_response = watsonx_model.generate_text(prompt=prompt, params=watsonx_params)\nwatsonx_output = watsonx_response\nprint(f\"Output: {watsonx_output}\\n\")\n\n# Compare outputs\nprint(\"=\" * 60)\nprint(\"\ud83d\udd0d Comparison:\")\nprint(f\"  Ollama length: {len(ollama_output)} chars\")\nprint(f\"  watsonx length: {len(watsonx_output)} chars\")\nprint(f\"  Exact match: {ollama_output == watsonx_output}\")\n\n# Calculate similarity (Levenshtein distance)\nfrom rapidfuzz.distance import Levenshtein\ndistance = Levenshtein.normalized_distance(ollama_output, watsonx_output)\nsimilarity = 1.0 - distance\nprint(f\"  Similarity: {similarity:.1%}\")\n\nif similarity &gt;= 0.95:\n    print(\"\\n\u2705 Cross-provider validation PASSED (\u226595% similarity)\")\nelse:\n    print(f\"\\n\u26a0\ufe0f  Cross-provider drift detected: {similarity:.1%}\")\n</code></pre> <p>Run it:</p> <pre><code>python test_cross_provider.py\n</code></pre> <p>Expected output (both Tier 1 models):</p> <pre><code>\ud83d\udd04 Cross-Provider Validation Test\n============================================================\nPrompt: Generate SQL to find all customers with account balance &gt; $100,000\n\n\ud83d\udccd Provider 1: Ollama (qwen2.5:7b-instruct)\nOutput: SELECT customer_name, account_balance FROM accounts WHERE account_balance &gt; 100000\n\n\u2601\ufe0f  Provider 2: watsonx.ai (granite-3-8b-instruct)\nOutput: SELECT customer_name, account_balance FROM accounts WHERE account_balance &gt; 100000\n\n============================================================\n\ud83d\udd0d Comparison:\n  Ollama length: 87 chars\n  watsonx length: 87 chars\n  Exact match: True\n  Similarity: 100.0%\n\n\u2705 Cross-provider validation PASSED (\u226595% similarity)\n</code></pre> <p>Tier 1 Cross-Provider Consistency</p> <p>Both Granite-3-8B (watsonx) and Qwen2.5-7B (Ollama) produce identical outputs\u2014enabling seamless migration between local and cloud deployments.</p>"},{"location":"lab-5/#step-3-use-the-frameworks-crossprovidervalidator","title":"Step 3: Use the Framework's CrossProviderValidator","text":"<p>Now use the built-in validator from <code>harness/cross_provider_validation.py</code>:</p> <p>Create <code>run_cross_provider_validation.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nUse framework's CrossProviderValidator for automated testing.\n\"\"\"\nfrom harness.cross_provider_validation import CrossProviderValidator\n\n# Initialize validator with GAAP materiality threshold\nvalidator = CrossProviderValidator(\n    providers=[\"ollama\", \"watsonx\"],\n    tolerance_pct=5.0  # \u00b15% from GAAP auditing standards\n)\n\n# SQL generation task\nprompt_sql = \"Generate SQL to find all customers with account balance &gt; $100,000\"\nresult_sql = validator.validate(\n    prompt=prompt_sql,\n    task_type=\"sql\",\n    model_ollama=\"qwen2.5:7b-instruct\",\n    model_watsonx=\"ibm/granite-3-8b-instruct\",\n    temperature=0.0,\n    seed=42\n)\n\nprint(\"\\n\ud83d\udcca SQL Generation Task\")\nprint(\"=\" * 60)\nprint(f\"Consistent: {result_sql['consistent']}\")\nprint(f\"Similarity: {result_sql['similarity']:.1%}\")\nprint(f\"Validation: {'\u2705 PASS' if result_sql['consistent'] else '\u274c FAIL'}\")\n\n# RAG task\nprompt_rag = \"What were Citigroup's net credit losses in 2023?\"\nresult_rag = validator.validate(\n    prompt=prompt_rag,\n    task_type=\"rag\",\n    model_ollama=\"qwen2.5:7b-instruct\",\n    model_watsonx=\"ibm/granite-3-8b-instruct\",\n    temperature=0.0,\n    seed=42\n)\n\nprint(\"\\n\ud83d\udcca RAG Task\")\nprint(\"=\" * 60)\nprint(f\"Consistent: {result_rag['consistent']}\")\nprint(f\"Similarity: {result_rag['similarity']:.1%}\")\nprint(f\"Factual consistency: {result_rag['factual_match']}\")\nprint(f\"Validation: {'\u2705 PASS' if result_rag['consistent'] else '\u26a0\ufe0f  MINOR DRIFT'}\")\n\n# Generate audit report\nprint(\"\\n\ud83d\udcc4 Cross-Provider Audit Report\")\nprint(\"=\" * 60)\nfor provider, output in result_sql['outputs'].items():\n    print(f\"{provider:15s}: {output[:80]}...\")\n</code></pre> <p>Run it:</p> <pre><code>python run_cross_provider_validation.py\n</code></pre>"},{"location":"lab-5/#step-4-gaap-materiality-threshold-5","title":"Step 4: GAAP Materiality Threshold (\u00b15%)","text":"<p>The framework uses \u00b15% tolerance based on GAAP auditing standards for financial statement materiality.</p> <p>Example: Numeric comparison</p> <pre><code>def validate_numeric_tolerance(value1: float, value2: float, tolerance_pct: float = 5.0) -&gt; bool:\n    \"\"\"Check if two values are within GAAP materiality threshold.\"\"\"\n    if value1 == 0 and value2 == 0:\n        return True\n    if value1 == 0 or value2 == 0:\n        return False\n\n    diff_pct = abs(value1 - value2) / max(value1, value2) * 100\n    return diff_pct &lt;= tolerance_pct\n\n# Test cases\nprint(validate_numeric_tolerance(2.4, 2.5, tolerance_pct=5.0))  # True (4.2% diff)\nprint(validate_numeric_tolerance(100, 110, tolerance_pct=5.0))  # False (9.1% diff)\nprint(validate_numeric_tolerance(1000, 1040, tolerance_pct=5.0))  # True (3.8% diff)\n</code></pre> <p>Why 5%? - GAAP materiality standard for financial reporting - Industry-accepted threshold for immaterial differences - Balances strictness with practical variance</p>"},{"location":"lab-5/#step-5-multi-run-cross-provider-test","title":"Step 5: Multi-Run Cross-Provider Test","text":"<p>Test consistency across multiple runs (n=5):</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nMulti-run cross-provider consistency test.\n\"\"\"\nfrom harness.cross_provider_validation import CrossProviderValidator\n\nvalidator = CrossProviderValidator(providers=[\"ollama\", \"watsonx\"], tolerance_pct=5.0)\nprompt = \"Generate SQL to find all customers with account balance &gt; $100,000\"\n\nresults = []\nfor i in range(1, 6):\n    result = validator.validate(\n        prompt=prompt,\n        task_type=\"sql\",\n        model_ollama=\"qwen2.5:7b-instruct\",\n        model_watsonx=\"ibm/granite-3-8b-instruct\",\n        temperature=0.0,\n        seed=42\n    )\n    results.append(result['consistent'])\n    print(f\"Run {i}: {'\u2705 Consistent' if result['consistent'] else '\u274c Inconsistent'}\")\n\nconsistency_rate = sum(results) / len(results) * 100\nprint(f\"\\nOverall consistency: {consistency_rate:.0f}%\")\n</code></pre> <p>Expected output:</p> <pre><code>Run 1: \u2705 Consistent\nRun 2: \u2705 Consistent\nRun 3: \u2705 Consistent\nRun 4: \u2705 Consistent\nRun 5: \u2705 Consistent\n\nOverall consistency: 100%\n</code></pre>"},{"location":"lab-5/#step-6-migration-decision-matrix","title":"Step 6: Migration Decision Matrix","text":"<p>Based on cross-provider validation, decide whether migration is safe:</p> Scenario Ollama \u2192 watsonx Validation Safe to Migrate? SQL (Tier 1 \u2192 Tier 1) Qwen2.5-7B \u2192 Granite-3-8B 100% match \u2705 Yes RAG (Tier 1 \u2192 Tier 1) Qwen2.5-7B \u2192 Granite-3-8B \u226595% match \u2705 Yes SQL (Tier 1 \u2192 Tier 2) Qwen2.5-7B \u2192 Llama-3.3-70B 100% match \u2705 Yes RAG (Tier 1 \u2192 Tier 2) Qwen2.5-7B \u2192 Llama-3.3-70B &lt;95% match \u26a0\ufe0f Monitor Any (Tier 1 \u2192 Tier 3) Qwen2.5-7B \u2192 GPT-OSS-120B &lt;50% match \u274c No <p>Migration safety check:</p> <pre><code>def is_migration_safe(source_tier: int, target_tier: int, task_type: str) -&gt; bool:\n    \"\"\"Check if migration between providers is compliance-safe.\"\"\"\n    if source_tier == 1 and target_tier == 1:\n        return True  # Always safe: Tier 1 \u2192 Tier 1\n\n    if target_tier == 3:\n        return False  # Never safe: Any \u2192 Tier 3\n\n    if target_tier == 2 and task_type in [\"sql\", \"summarize\"]:\n        return True  # Safe for structured tasks\n\n    return False  # Requires validation\n\n# Examples\nprint(is_migration_safe(1, 1, \"rag\"))  # True\nprint(is_migration_safe(1, 2, \"sql\"))  # True\nprint(is_migration_safe(1, 2, \"rag\"))  # False (requires validation)\nprint(is_migration_safe(1, 3, \"sql\"))  # False\n</code></pre>"},{"location":"lab-5/#understanding-provider-differences","title":"Understanding Provider Differences","text":"<p>Even with identical model versions, providers may differ in:</p> <ol> <li>Infrastructure: GPU hardware, CUDA versions</li> <li>Quantization: Different precision (FP16, FP32, INT8)</li> <li>Batching: Request handling and parallelization</li> <li>Load balancing: Multiple model replicas</li> </ol> <p>Tier 1 Advantage</p> <p>Tier 1 models (7-8B) are small enough to fit on a single GPU consistently, reducing infrastructure-induced variance.</p>"},{"location":"lab-5/#troubleshooting","title":"Troubleshooting","text":""},{"location":"lab-5/#watsonxai-connection-issues","title":"watsonx.ai Connection Issues","text":"<pre><code># Test watsonx.ai connectivity\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nimport os\n\ntry:\n    model = ModelInference(\n        model_id=\"ibm/granite-3-8b-instruct\",\n        api_key=os.getenv(\"WATSONX_API_KEY\"),\n        project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n        url=os.getenv(\"WATSONX_URL\")\n    )\n    print(\"\u2705 watsonx.ai connection successful\")\nexcept Exception as e:\n    print(f\"\u274c watsonx.ai connection failed: {e}\")\n</code></pre>"},{"location":"lab-5/#similarity-below-95","title":"Similarity Below 95%","text":"<p>If cross-provider similarity is unexpectedly low:</p> <ol> <li>Check model versions: Ensure same base model</li> <li>Verify temperature: Must be exactly 0.0</li> <li>Use explicit seeds: Set <code>seed=42</code> for both</li> <li>Inspect raw outputs: Look for formatting differences</li> </ol> <pre><code># Debug output differences\nprint(\"Ollama output:\", repr(ollama_output))\nprint(\"watsonx output:\", repr(watsonx_output))\n</code></pre>"},{"location":"lab-5/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Cross-provider validation ensures migration safety</li> <li>Tier 1 models (7-8B) achieve perfect cross-provider consistency</li> <li>GAAP materiality (\u00b15%) provides finance-calibrated tolerance</li> <li>Framework's <code>CrossProviderValidator</code> automates testing</li> <li>Audit trails document cross-provider equivalence</li> </ol>"},{"location":"lab-5/#quiz-test-your-understanding","title":"Quiz: Test Your Understanding","text":"What is the GAAP materiality threshold used in cross-provider validation? <p>Answer: \u00b15%, based on GAAP auditing standards for financial statement materiality.</p> Why do Tier 1 models show better cross-provider consistency? <p>Answer: They're small enough (7-8B params) to fit on a single GPU, reducing infrastructure-induced variance from distributed processing.</p> When is migration from Tier 1 to Tier 2 safe? <p>Answer: Only for structured tasks (SQL, summarization). RAG tasks require explicit validation due to Tier 2's lower RAG consistency.</p>"},{"location":"lab-5/#next-steps","title":"Next Steps","text":"<p>Now that you understand cross-provider validation:</p> <ol> <li>Proceed to Lab 6: Extending the Framework to add custom tasks</li> <li>Test your own provider combinations</li> <li>Review <code>harness/cross_provider_validation.py</code> for implementation details</li> </ol> <p>Lab 5 Complete!</p> <p>You can now validate cross-provider consistency and make migration decisions with confidence! Ready to customize the framework? Move on to Lab 6: Extending the Framework!</p>"},{"location":"lab-6/","title":"Lab 6: Extending the Framework","text":""},{"location":"lab-6/#overview","title":"Overview","text":"<p>In this lab, you'll learn how to customize the framework for your own use cases by adding new tasks, modifying prompt templates, and integrating with your workflows.</p> <p>Duration: ~30 minutes</p>"},{"location":"lab-6/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you will:</p> <ul> <li>Add custom tasks to <code>prompts/templates.json</code></li> <li>Modify existing prompts for your domain</li> <li>Integrate the framework into CI/CD pipelines</li> <li>Create custom compliance validators</li> <li>Export results for regulatory reporting</li> </ul>"},{"location":"lab-6/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed Lab 5: Cross-Provider Testing</li> <li>Understanding of JSON structure</li> <li>Familiarity with your organization's compliance requirements</li> </ul>"},{"location":"lab-6/#framework-architecture","title":"Framework Architecture","text":"<p>The framework is designed for extensibility:</p> <pre><code>output-drift-financial-llms/\n\u251c\u2500\u2500 prompts/\n\u2502   \u2514\u2500\u2500 templates.json          # \u2190 Add your custom tasks here\n\u251c\u2500\u2500 harness/\n\u2502   \u251c\u2500\u2500 task_definitions.py     # Task execution logic\n\u2502   \u251c\u2500\u2500 deterministic_retriever.py\n\u2502   \u2514\u2500\u2500 cross_provider_validation.py\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sec_filings/            # \u2190 Add your own documents\n\u2502   \u2514\u2500\u2500 toy_finance.sqlite      # \u2190 Use your database\n\u2514\u2500\u2500 examples/                   # \u2190 Reference implementations\n</code></pre>"},{"location":"lab-6/#step-1-understanding-template-structure","title":"Step 1: Understanding Template Structure","text":"<p>Open <code>prompts/templates.json</code> to see the existing tasks:</p> <pre><code>cat prompts/templates.json\n</code></pre> <p>Current tasks: - <code>rag</code>: RAG Q&amp;A over SEC 10-K filings - <code>summary</code>: JSON summarization with schema validation - <code>sql</code>: Text-to-SQL generation</p> <p>Each task has: - description: What the task does - prompts: Array of test cases - system_prompt: Instructions for the LLM - temperature: 0.0 for determinism - seed: 42 for reproducibility</p>"},{"location":"lab-6/#step-2-add-a-custom-task-credit-risk-analysis","title":"Step 2: Add a Custom Task - Credit Risk Analysis","text":"<p>Let's add a new task for credit risk assessment:</p> <p>Edit <code>prompts/templates.json</code> and add after the <code>sql</code> section:</p> <pre><code>{\n  \"credit_risk\": {\n    \"description\": \"Credit risk classification with explainability requirements\",\n    \"prompts\": [\n      {\n        \"id\": \"cr1\",\n        \"profile\": {\n          \"credit_score\": 680,\n          \"income\": 75000,\n          \"debt_to_income\": 0.20,\n          \"employment_years\": 5\n        },\n        \"question\": \"Classify credit risk (LOW/MEDIUM/HIGH) and explain in one sentence.\",\n        \"expected_risk\": \"LOW\",\n        \"compliance_requirements\": [\"ECOA\", \"FCRA\"]\n      },\n      {\n        \"id\": \"cr2\",\n        \"profile\": {\n          \"credit_score\": 620,\n          \"income\": 50000,\n          \"debt_to_income\": 0.45,\n          \"employment_years\": 1\n        },\n        \"question\": \"Classify credit risk (LOW/MEDIUM/HIGH) and explain in one sentence.\",\n        \"expected_risk\": \"MEDIUM\",\n        \"compliance_requirements\": [\"ECOA\", \"FCRA\"]\n      }\n    ],\n    \"system_prompt\": \"You are a fair and consistent credit risk analyst. Classify risk as LOW, MEDIUM, or HIGH. Provide a brief explanation in one sentence. Be consistent: identical inputs must always produce identical outputs for regulatory compliance.\",\n    \"output_schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"risk_level\": {\"type\": \"string\", \"enum\": [\"LOW\", \"MEDIUM\", \"HIGH\"]},\n        \"explanation\": {\"type\": \"string\"}\n      },\n      \"required\": [\"risk_level\", \"explanation\"]\n    },\n    \"temperature\": 0.0,\n    \"seed\": 42\n  }\n}\n</code></pre>"},{"location":"lab-6/#step-3-create-task-executor-for-custom-task","title":"Step 3: Create Task Executor for Custom Task","text":"<p>Create <code>custom_credit_risk.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nCustom credit risk classification task with drift testing.\n\"\"\"\nimport json\nfrom openai import OpenAI\n\n# Load custom task template\nwith open(\"prompts/templates.json\") as f:\n    templates = json.load(f)\n    credit_risk_task = templates[\"credit_risk\"]\n\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\ndef run_credit_risk_assessment(profile: dict, model: str = \"qwen2.5:7b-instruct\", n_runs: int = 5):\n    \"\"\"Run credit risk assessment n times to test consistency.\"\"\"\n    prompt = f\"\"\"Profile:\n- Credit Score: {profile['credit_score']}\n- Annual Income: ${profile['income']:,}\n- Debt-to-Income Ratio: {profile['debt_to_income']:.0%}\n- Employment Years: {profile['employment_years']}\n\n{credit_risk_task['prompts'][0]['question']}\"\"\"\n\n    results = []\n    for i in range(1, n_runs + 1):\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": credit_risk_task['system_prompt']},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.0,\n            seed=42\n        )\n        output = response.choices[0].message.content\n        results.append(output)\n        print(f\"Run {i}: {output}\")\n\n    # Check consistency\n    unique = len(set(results))\n    consistency = (1 / unique) * 100 if unique &gt; 0 else 100.0\n\n    print(f\"\\n\ud83d\udcca Results:\")\n    print(f\"  Total runs: {n_runs}\")\n    print(f\"  Unique outputs: {unique}\")\n    print(f\"  Consistency: {consistency:.0f}%\")\n    print(f\"  Status: {'\u2705 Audit-ready' if consistency == 100 else '\u26a0\ufe0f Drift detected'}\")\n\n    return results\n\n# Test with the first profile\nprofile1 = credit_risk_task['prompts'][0]['profile']\nprint(\"\ud83e\uddea Testing Credit Risk Assessment\\n\")\nresults = run_credit_risk_assessment(profile1, n_runs=5)\n</code></pre> <p>Run it:</p> <pre><code>python custom_credit_risk.py\n</code></pre> <p>Expected output (Tier 1 model):</p> <pre><code>\ud83e\uddea Testing Credit Risk Assessment\n\nRun 1: {\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio and stable employment history.\"}\nRun 2: {\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio and stable employment history.\"}\nRun 3: {\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio and stable employment history.\"}\nRun 4: {\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio and stable employment history.\"}\nRun 5: {\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio and stable employment history.\"}\n\n\ud83d\udcca Results:\n  Total runs: 5\n  Unique outputs: 1\n  Consistency: 100%\n  Status: \u2705 Audit-ready\n</code></pre>"},{"location":"lab-6/#step-4-add-domain-specific-documents","title":"Step 4: Add Domain-Specific Documents","text":"<p>To use RAG with your own documents:</p> <ol> <li>Add documents to <code>data/sec_filings/</code> (or create a new folder):</li> </ol> <pre><code>mkdir -p data/custom_docs\n</code></pre> <ol> <li>Update <code>deterministic_retriever.py</code> to point to your folder:</li> </ol> <pre><code>from harness.deterministic_retriever import DeterministicRetriever\n\nretriever = DeterministicRetriever(\n    corpus_path=\"data/custom_docs/\",  # Your documents here\n    chunk_size=512,\n    overlap=50\n)\n</code></pre> <ol> <li>Test retrieval:</li> </ol> <pre><code>query = \"What is our company's annual revenue?\"\nresults = retriever.retrieve(query, top_k=5)\nfor i, chunk in enumerate(results, 1):\n    print(f\"Chunk {i}: {chunk['text'][:100]}...\")\n</code></pre>"},{"location":"lab-6/#step-5-cicd-integration","title":"Step 5: CI/CD Integration","text":"<p>Integrate drift testing into your CI/CD pipeline:</p> <p>Create <code>.github/workflows/drift-test.yml</code>:</p> <pre><code>name: LLM Output Drift Testing\n\non:\n  pull_request:\n    branches: [main]\n  schedule:\n    - cron: '0 0 * * *'  # Daily at midnight\n\njobs:\n  drift-test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Run drift evaluation\n        env:\n          WATSONX_API_KEY: ${{ secrets.WATSONX_API_KEY }}\n          WATSONX_PROJECT_ID: ${{ secrets.WATSONX_PROJECT_ID }}\n        run: |\n          python run_evaluation.py \\\n            --model ibm/granite-3-8b-instruct \\\n            --temperature 0.0 \\\n            --concurrency 16 \\\n            --task sql \\\n            --output traces/ci_test.jsonl\n\n      - name: Validate consistency\n        run: |\n          python -c \"\n          import json\n          with open('traces/ci_test.jsonl') as f:\n              data = [json.loads(line) for line in f]\n          unique = len(set(d['response_hash'] for d in data))\n          assert unique == 1, f'Drift detected: {unique} unique outputs'\n          print('\u2705 Consistency check passed')\n          \"\n\n      - name: Upload audit trail\n        uses: actions/upload-artifact@v3\n        with:\n          name: drift-test-results\n          path: traces/ci_test.jsonl\n</code></pre> <p>This pipeline: - Runs on every PR and daily - Tests for drift with n=16 - Fails CI if drift detected - Uploads audit trails as artifacts</p>"},{"location":"lab-6/#step-6-custom-compliance-validator","title":"Step 6: Custom Compliance Validator","text":"<p>Create a validator for your specific regulations:</p> <p>Create <code>custom_compliance_validator.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nCustom compliance validator for specific regulatory frameworks.\n\"\"\"\nimport json\nfrom typing import Dict, List\n\nclass CustomComplianceValidator:\n    \"\"\"\n    Validate LLM outputs against custom compliance requirements.\n    \"\"\"\n\n    def __init__(self, frameworks: List[str]):\n        \"\"\"\n        Initialize validator.\n\n        Args:\n            frameworks: List of compliance frameworks (e.g., [\"ECOA\", \"FCRA\", \"GDPR\"])\n        \"\"\"\n        self.frameworks = frameworks\n        self.rules = self._load_rules()\n\n    def _load_rules(self) -&gt; Dict[str, callable]:\n        \"\"\"Load validation rules for each framework.\"\"\"\n        rules = {}\n\n        # ECOA (Equal Credit Opportunity Act)\n        if \"ECOA\" in self.frameworks:\n            rules[\"ecoa_consistency\"] = self._check_consistency\n            rules[\"ecoa_no_discrimination\"] = self._check_no_discrimination\n\n        # FCRA (Fair Credit Reporting Act)\n        if \"FCRA\" in self.frameworks:\n            rules[\"fcra_explainability\"] = self._check_explainability\n\n        # GDPR\n        if \"GDPR\" in self.frameworks:\n            rules[\"gdpr_right_to_explanation\"] = self._check_explainability\n            rules[\"gdpr_data_minimization\"] = self._check_data_minimization\n\n        return rules\n\n    def _check_consistency(self, outputs: List[str]) -&gt; bool:\n        \"\"\"ECOA: Similar applicants must receive similar treatment.\"\"\"\n        unique_outputs = len(set(outputs))\n        return unique_outputs == 1  # 100% consistency required\n\n    def _check_no_discrimination(self, output: str) -&gt; bool:\n        \"\"\"ECOA: No references to protected classes.\"\"\"\n        protected_terms = [\"race\", \"gender\", \"age\", \"religion\", \"nationality\"]\n        return not any(term in output.lower() for term in protected_terms)\n\n    def _check_explainability(self, output: str) -&gt; bool:\n        \"\"\"FCRA/GDPR: Must include explanation.\"\"\"\n        return \"explanation\" in output.lower() or \"because\" in output.lower()\n\n    def _check_data_minimization(self, output: str) -&gt; bool:\n        \"\"\"GDPR: Don't expose unnecessary personal data.\"\"\"\n        pii_indicators = [\"ssn\", \"social security\", \"passport\", \"driver license\"]\n        return not any(indicator in output.lower() for indicator in pii_indicators)\n\n    def validate(self, outputs: List[str]) -&gt; Dict[str, any]:\n        \"\"\"\n        Run all validation rules.\n\n        Args:\n            outputs: List of LLM outputs to validate\n\n        Returns:\n            {\n                \"compliant\": bool,\n                \"passed_rules\": List[str],\n                \"failed_rules\": List[str],\n                \"details\": Dict[str, bool]\n            }\n        \"\"\"\n        results = {}\n        for rule_name, rule_func in self.rules.items():\n            if rule_name.endswith(\"_consistency\"):\n                results[rule_name] = rule_func(outputs)\n            else:\n                # Check all outputs\n                results[rule_name] = all(rule_func(output) for output in outputs)\n\n        passed = [k for k, v in results.items() if v]\n        failed = [k for k, v in results.items() if not v]\n\n        return {\n            \"compliant\": len(failed) == 0,\n            \"passed_rules\": passed,\n            \"failed_rules\": failed,\n            \"details\": results\n        }\n\n# Example usage\nvalidator = CustomComplianceValidator(frameworks=[\"ECOA\", \"FCRA\"])\n\n# Test outputs from credit risk assessment\ntest_outputs = [\n    '{\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio.\"}',\n    '{\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio.\"}',\n    '{\"risk_level\": \"LOW\", \"explanation\": \"Strong credit profile with good income-to-debt ratio.\"}'\n]\n\nresult = validator.validate(test_outputs)\n\nprint(\"\\n\ud83d\udccb Compliance Validation Report\")\nprint(\"=\" * 60)\nprint(f\"Compliant: {'\u2705 YES' if result['compliant'] else '\u274c NO'}\")\nprint(f\"\\nPassed rules ({len(result['passed_rules'])}):\")\nfor rule in result['passed_rules']:\n    print(f\"  \u2705 {rule}\")\nif result['failed_rules']:\n    print(f\"\\nFailed rules ({len(result['failed_rules'])}):\")\n    for rule in result['failed_rules']:\n        print(f\"  \u274c {rule}\")\n</code></pre> <p>Run it:</p> <pre><code>python custom_compliance_validator.py\n</code></pre>"},{"location":"lab-6/#step-7-export-for-regulatory-reporting","title":"Step 7: Export for Regulatory Reporting","text":"<p>Generate compliance reports from audit trails:</p> <p>Create <code>generate_compliance_report.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nGenerate compliance report from audit trails.\n\"\"\"\nimport json\nimport pandas as pd\nfrom datetime import datetime\n\ndef generate_report(trace_file: str, output_format: str = \"html\"):\n    \"\"\"Generate compliance report from JSONL audit trail.\"\"\"\n\n    # Load audit trail\n    with open(trace_file) as f:\n        traces = [json.loads(line) for line in f]\n\n    # Calculate metrics\n    total_runs = len(traces)\n    unique_outputs = len(set(t['response_hash'] for t in traces))\n    consistency = (unique_outputs == 1)\n    consistency_pct = (1 / unique_outputs * 100) if unique_outputs &gt; 0 else 100.0\n\n    # Compliance metrics\n    schema_violations = sum(not t['compliance_metrics']['schema_valid'] for t in traces)\n    decision_flips = sum(t['compliance_metrics']['decision_flip'] for t in traces)\n    mean_drift = sum(t['compliance_metrics']['factual_drift'] for t in traces) / total_runs\n\n    # Generate HTML report\n    html = f\"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;LLM Compliance Report&lt;/title&gt;\n        &lt;style&gt;\n            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n            h1 {{ color: #0f62fe; }}\n            .metric {{ padding: 10px; margin: 10px 0; border-left: 4px solid #0f62fe; background: #f4f4f4; }}\n            .pass {{ border-left-color: #24a148; }}\n            .fail {{ border-left-color: #da1e28; }}\n            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n            th {{ background-color: #0f62fe; color: white; }}\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h1&gt;LLM Output Drift Compliance Report&lt;/h1&gt;\n        &lt;p&gt;&lt;strong&gt;Generated:&lt;/strong&gt; {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}&lt;/p&gt;\n        &lt;p&gt;&lt;strong&gt;Audit Trail:&lt;/strong&gt; {trace_file}&lt;/p&gt;\n\n        &lt;h2&gt;Executive Summary&lt;/h2&gt;\n        &lt;div class=\"metric {'pass' if consistency else 'fail'}\"&gt;\n            &lt;strong&gt;Consistency:&lt;/strong&gt; {consistency_pct:.1f}% ({unique_outputs} unique output{'s' if unique_outputs != 1 else ''})\n        &lt;/div&gt;\n        &lt;div class=\"metric {'pass' if schema_violations == 0 else 'fail'}\"&gt;\n            &lt;strong&gt;Schema Violations:&lt;/strong&gt; {schema_violations}\n        &lt;/div&gt;\n        &lt;div class=\"metric {'pass' if decision_flips == 0 else 'fail'}\"&gt;\n            &lt;strong&gt;Decision Flips:&lt;/strong&gt; {decision_flips}\n        &lt;/div&gt;\n        &lt;div class=\"metric {'pass' if mean_drift &lt; 0.05 else 'fail'}\"&gt;\n            &lt;strong&gt;Mean Drift:&lt;/strong&gt; {mean_drift:.3f}\n        &lt;/div&gt;\n\n        &lt;h2&gt;Regulatory Compliance Status&lt;/h2&gt;\n        &lt;table&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Requirement&lt;/th&gt;\n                &lt;th&gt;Status&lt;/th&gt;\n                &lt;th&gt;Evidence&lt;/th&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;SR 11-7 (Model Validation)&lt;/td&gt;\n                &lt;td&gt;{'\u2705 PASS' if consistency else '\u274c FAIL'}&lt;/td&gt;\n                &lt;td&gt;Deterministic behavior: {consistency_pct:.1f}%&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;ECOA (Consistent Decisions)&lt;/td&gt;\n                &lt;td&gt;{'\u2705 PASS' if decision_flips == 0 else '\u274c FAIL'}&lt;/td&gt;\n                &lt;td&gt;Decision flips: {decision_flips}&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;FSB (Output Consistency)&lt;/td&gt;\n                &lt;td&gt;{'\u2705 PASS' if mean_drift &lt; 0.05 else '\u274c FAIL'}&lt;/td&gt;\n                &lt;td&gt;Mean drift: {mean_drift:.3f}&lt;/td&gt;\n            &lt;/tr&gt;\n        &lt;/table&gt;\n\n        &lt;h2&gt;Model Configuration&lt;/h2&gt;\n        &lt;pre&gt;{json.dumps(traces[0], indent=2)[:500]}...&lt;/pre&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    # Save report\n    output_file = trace_file.replace('.jsonl', '_compliance_report.html')\n    with open(output_file, 'w') as f:\n        f.write(html)\n\n    print(f\"\u2705 Compliance report generated: {output_file}\")\n    return output_file\n\n# Generate report\ngenerate_report(\"traces/lab3_sql.jsonl\")\n</code></pre> <p>Run it:</p> <pre><code>python generate_compliance_report.py\n</code></pre> <p>Open the HTML report in your browser to see a formatted compliance report.</p>"},{"location":"lab-6/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Templates are JSON - Easy to add custom tasks</li> <li>Modular design - Extend components independently</li> <li>CI/CD ready - Integrate into deployment pipelines</li> <li>Custom validators - Implement your regulatory requirements</li> <li>Exportable reports - Generate audit documentation</li> </ol>"},{"location":"lab-6/#best-practices-for-extensions","title":"Best Practices for Extensions","text":"<ol> <li>Always test with n\u226516 to detect drift</li> <li>Use T=0.0 and explicit seeds for determinism</li> <li>Document compliance mappings in audit trails</li> <li>Version your prompts (metadata section)</li> <li>Validate cross-provider before production deployment</li> </ol>"},{"location":"lab-6/#quiz-test-your-understanding","title":"Quiz: Test Your Understanding","text":"Where do you add custom tasks? <p>Answer: <code>prompts/templates.json</code> - add a new top-level key with task configuration.</p> What's the minimum number of runs recommended for drift testing? <p>Answer: 16 (n=16), as used in the paper's methodology.</p> How do you ensure determinism in custom tasks? <p>Answer: Set <code>temperature: 0.0</code> and <code>seed: 42</code> in the template, and test consistency with multiple runs.</p>"},{"location":"lab-6/#next-steps","title":"Next Steps","text":"<p>You've completed all workshop labs! Now you can:</p> <ol> <li>Review API Reference for detailed documentation</li> <li>Check Troubleshooting Guide for common issues</li> <li>Read the full research paper</li> <li>Contribute improvements via GitHub</li> </ol> <p>Lab 6 Complete! \ud83c\udf89</p> <p>You've completed the entire workshop! You can now measure drift, classify models, validate cross-provider consistency, and extend the framework for your use cases. Thank you for participating!</p>"},{"location":"pre-work/","title":"Lab 0: Workshop Pre-Work","text":"<p>Welcome to the Output Drift workshop! This lab will guide you through setting up your environment and ensuring you have all the prerequisites installed.</p>"},{"location":"pre-work/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following installed on your system:</p>"},{"location":"pre-work/#required-software","title":"Required Software","text":"<ol> <li>Python 3.11 or higher</li> </ol> <p>Check your Python version:    <pre><code>python --version\n# or\npython3 --version\n</code></pre></p> <p>If you need to install Python, visit python.org</p> <ol> <li>Git</li> </ol> <p>Check if Git is installed:    <pre><code>git --version\n</code></pre></p> <p>If you need to install Git, visit git-scm.com</p> <ol> <li>Text Editor or IDE</li> </ol> <p>We recommend:    - VS Code    - PyCharm    - Jupyter Notebook/Lab    - Or any editor of your choice</p>"},{"location":"pre-work/#optional-but-recommended","title":"Optional (but Recommended)","text":"<ol> <li>Ollama (for free, local LLM testing)</li> </ol> <p>Install from ollama.ai</p> <p>After installation, verify:    <pre><code>ollama --version\n</code></pre></p> <p>Pull a recommended model:    <pre><code>ollama pull qwen2.5:7b-instruct\n</code></pre></p>"},{"location":"pre-work/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Clone the workshop repository to your local machine:</p> <pre><code>git clone https://github.com/ibm-client-engineering/output-drift-financial-llms\ncd output-drift-financial-llms\n</code></pre> <p>For reproducibility, checkout the v0.1.0 release:</p> <pre><code>git checkout v0.1.0\n</code></pre>"},{"location":"pre-work/#step-2-set-up-a-virtual-environment","title":"Step 2: Set Up a Virtual Environment","text":"<p>Create and activate a Python virtual environment:</p> macOS/LinuxWindows <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre> <pre><code>python -m venv venv\nvenv\\Scripts\\activate\n</code></pre> <p>You should see <code>(venv)</code> in your terminal prompt, indicating the virtual environment is active.</p>"},{"location":"pre-work/#step-3-install-dependencies","title":"Step 3: Install Dependencies","text":"<p>Install all required Python packages:</p> <pre><code>pip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <p>This will install: - <code>ollama</code> - Ollama API client for local models - <code>pandas</code>, <code>numpy</code> - Data analysis and numerical computing - <code>matplotlib</code>, <code>scipy</code> - Visualization and statistical analysis - <code>rapidfuzz</code>, <code>python-Levenshtein</code> - Drift detection metrics - <code>sec-edgar-downloader</code>, <code>beautifulsoup4</code> - SEC filing data - <code>faker</code> - Synthetic financial data generation - <code>python-dotenv</code> - Environment variable management - <code>pytest</code> - Testing framework - And other dependencies (see requirements.txt for full list)</p>"},{"location":"pre-work/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<p>Test that the framework can import correctly:</p> <pre><code>python -c \"from harness.task_definitions import TaskDefinition; print('\u2705 Installation successful!')\"\n</code></pre> <p>If you see \"\u2705 Installation successful!\", you're all set!</p>"},{"location":"pre-work/#step-5-set-up-api-keys-optional","title":"Step 5: Set Up API Keys (Optional)","text":"<p>To run experiments with cloud providers, you'll need API keys. Don't worry if you don't have all of these\u2014you can start with Ollama (free and local) and add others later.</p>"},{"location":"pre-work/#create-a-env-file","title":"Create a <code>.env</code> File","text":"<p>In the repository root, create a <code>.env</code> file:</p> <pre><code>touch .env\n</code></pre>"},{"location":"pre-work/#add-your-api-keys","title":"Add Your API Keys","text":"<p>Edit the <code>.env</code> file and add the keys you have:</p> <pre><code># Ollama (local, no key needed)\nOLLAMA_BASE_URL=http://localhost:11434\n\n# IBM watsonx.ai (if you have access)\nWATSONX_API_KEY=your_watsonx_api_key_here\nWATSONX_PROJECT_ID=your_project_id_here\n\n# OpenAI (if you have access)\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Anthropic (if you have access)\nANTHROPIC_API_KEY=your_anthropic_api_key_here\n</code></pre> <p>Keep Your Keys Secret</p> <p>Never commit your <code>.env</code> file to Git! It's already in <code>.gitignore</code> to prevent accidental commits.</p>"},{"location":"pre-work/#where-to-get-api-keys","title":"Where to Get API Keys","text":"Ollama (Free)IBM watsonx.aiOpenAIAnthropic <p>No API key needed! Just install and run locally.</p> <pre><code># Install from https://ollama.ai/\nollama serve\n</code></pre> <ol> <li>Visit watsonx.ai</li> <li>Sign up for a trial or use your IBM Cloud account</li> <li>Create a project and get your API key and project ID</li> <li>Setup Guide</li> </ol> <ol> <li>Visit platform.openai.com</li> <li>Sign up and navigate to API keys</li> <li>Create a new API key</li> <li>Add billing information (required for API access)</li> </ol> <ol> <li>Visit console.anthropic.com</li> <li>Sign up and navigate to API keys</li> <li>Create a new API key</li> <li>Add billing information</li> </ol>"},{"location":"pre-work/#step-6-test-your-setup","title":"Step 6: Test Your Setup","text":"<p>Test your providers to verify everything is configured correctly:</p>"},{"location":"pre-work/#test-ollama-local","title":"Test Ollama (Local)","text":"<p>If you installed Ollama:</p> <pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/tags\n\n# Test with a simple query (requires qwen2.5:7b-instruct)\npython -c \"\nfrom openai import OpenAI\nclient = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\nresponse = client.chat.completions.create(\n    model='qwen2.5:7b-instruct',\n    messages=[{'role': 'user', 'content': 'Say hello!'}],\n    temperature=0.0\n)\nprint('\u2705 Ollama is working!')\nprint(f'Response: {response.choices[0].message.content}')\n\"\n</code></pre>"},{"location":"pre-work/#test-watsonxai-optional","title":"Test watsonx.ai (Optional)","text":"<p>If you configured watsonx.ai:</p> <pre><code>python -c \"\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nprint('\u2705 watsonx API key:', 'configured' if os.getenv('WATSONX_API_KEY') else 'NOT configured')\nprint('\u2705 watsonx Project ID:', 'configured' if os.getenv('WATSONX_PROJECT_ID') else 'NOT configured')\n\"\n</code></pre> <p>At Least One Provider</p> <p>You need at least one provider configured (even just Ollama) to complete the workshop!</p>"},{"location":"pre-work/#troubleshooting","title":"Troubleshooting","text":""},{"location":"pre-work/#python-version-issues","title":"Python Version Issues","text":"<p>If you have multiple Python versions installed:</p> <pre><code># Use python3.11 explicitly\npython3.11 -m venv venv\n</code></pre>"},{"location":"pre-work/#ollama-connection-issues","title":"Ollama Connection Issues","text":"<p>If Ollama isn't responding:</p> <pre><code># Start Ollama server\nollama serve\n\n# In another terminal, test:\ncurl http://localhost:11434/api/tags\n</code></pre>"},{"location":"pre-work/#import-errors","title":"Import Errors","text":"<p>If you see import errors, ensure you're in the virtual environment:</p> <pre><code># Check if venv is activated\nwhich python\n# Should show: /path/to/your/repo/venv/bin/python\n\n# If not, activate it:\nsource venv/bin/activate  # macOS/Linux\n# or\nvenv\\Scripts\\activate  # Windows\n</code></pre>"},{"location":"pre-work/#permission-errors-on-macoslinux","title":"Permission Errors on macOS/Linux","text":"<p>If you encounter permission errors:</p> <pre><code># Use --user flag\npip install --user -r requirements.txt\n</code></pre>"},{"location":"pre-work/#next-steps","title":"Next Steps","text":"<p>Once your environment is set up and tested:</p> <ol> <li>Proceed to Lab 1: Understanding Output Drift</li> <li>Explore the <code>examples/</code> directory for sample configurations</li> <li>Review the research paper in <code>docs/resources/paper.md</code></li> </ol>"},{"location":"pre-work/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Ask workshop facilitators</li> <li>Open an issue on GitHub</li> </ol> <p>Ready?</p> <p>If all tests pass, you're ready to move on to Lab 1: Understanding Output Drift!</p>"},{"location":"resources/api/","title":"API Reference","text":"<p>Complete reference documentation for the Output Drift framework components.</p>"},{"location":"resources/api/#core-classes","title":"Core Classes","text":""},{"location":"resources/api/#deterministicretriever","title":"DeterministicRetriever","text":"<p>Location: <code>harness/deterministic_retriever.py</code></p> <p>Ensures reproducible SEC 10-K retrieval with multi-key ordering.</p> <pre><code>from harness.deterministic_retriever import DeterministicRetriever\n\nretriever = DeterministicRetriever(\n    corpus_path=\"data/sec_filings/\",\n    chunk_size=512,\n    overlap=50\n)\n</code></pre> <p>Methods:</p>"},{"location":"resources/api/#retrievequery-top_k5","title":"<code>retrieve(query, top_k=5)</code>","text":"<p>Retrieve top-k chunks with deterministic ordering.</p> <p>Parameters: - <code>query</code> (str): Search query - <code>top_k</code> (int, default=5): Number of chunks to return</p> <p>Returns: - List[Dict]: Chunks with keys <code>[\"snippet_id\", \"text\", \"score\", \"source\", \"metadata\"]</code></p> <p>Example: <pre><code>results = retriever.retrieve(\"What were net credit losses?\", top_k=5)\nfor chunk in results:\n    print(f\"{chunk['source']}: {chunk['text'][:100]}...\")\n</code></pre></p>"},{"location":"resources/api/#crossprovidervalidator","title":"CrossProviderValidator","text":"<p>Location: <code>harness/cross_provider_validation.py</code></p> <p>Validates consistency across local (Ollama) and cloud (watsonx.ai) providers.</p> <pre><code>from harness.cross_provider_validation import CrossProviderValidator\n\nvalidator = CrossProviderValidator(\n    providers=[\"ollama\", \"watsonx\"],\n    tolerance_pct=5.0  # GAAP materiality threshold\n)\n</code></pre> <p>Methods:</p>"},{"location":"resources/api/#validateprompt-task_type-kwargs","title":"<code>validate(prompt, task_type, **kwargs)</code>","text":"<p>Validate output consistency across providers.</p> <p>Parameters: - <code>prompt</code> (str): Input prompt - <code>task_type</code> (str): One of <code>\"rag\"</code>, <code>\"sql\"</code>, <code>\"summary\"</code> - <code>**kwargs</code>: Provider-specific configs (model, temperature, seed)</p> <p>Returns: - Dict with keys:     - <code>consistent</code> (bool): Whether outputs match     - <code>outputs</code> (Dict[str, str]): Provider \u2192 output mapping     - <code>similarity</code> (float): Normalized similarity score (0.0-1.0)     - <code>factual_match</code> (bool): Factual consistency (for RAG)</p> <p>Example: <pre><code>result = validator.validate(\n    prompt=\"Generate SQL to find customers with balance &gt; $100k\",\n    task_type=\"sql\",\n    model_ollama=\"qwen2.5:7b-instruct\",\n    model_watsonx=\"ibm/granite-3-8b-instruct\",\n    temperature=0.0,\n    seed=42\n)\nprint(f\"Consistent: {result['consistent']}\")\nprint(f\"Similarity: {result['similarity']:.1%}\")\n</code></pre></p>"},{"location":"resources/api/#task-definitions","title":"Task Definitions","text":"<p>Location: <code>prompts/templates.json</code></p>"},{"location":"resources/api/#rag-task","title":"RAG Task","text":"<pre><code>{\n  \"rag\": {\n    \"description\": \"RAG Q&amp;A over SEC 10-K filings with citation validation\",\n    \"prompts\": [...],\n    \"system_prompt\": \"You are a precise financial analyst...\",\n    \"temperature\": 0.0,\n    \"seed\": 42\n  }\n}\n</code></pre>"},{"location":"resources/api/#sql-task","title":"SQL Task","text":"<pre><code>{\n  \"sql\": {\n    \"description\": \"Text-to-SQL with invariant checking (\u00b15% GAAP materiality)\",\n    \"prompts\": [...],\n    \"system_prompt\": \"You write SQLite SQL ONLY...\",\n    \"schema_description\": \"transactions(id INT, date TEXT, region TEXT, amount REAL, category TEXT)\",\n    \"temperature\": 0.0,\n    \"seed\": 42\n  }\n}\n</code></pre>"},{"location":"resources/api/#summarization-task","title":"Summarization Task","text":"<pre><code>{\n  \"summary\": {\n    \"description\": \"Policy-bounded JSON summarization with schema constraints\",\n    \"prompts\": [...],\n    \"system_prompt\": \"You produce STRICT JSON...\",\n    \"schema\": {...},\n    \"temperature\": 0.0,\n    \"seed\": 42\n  }\n}\n</code></pre>"},{"location":"resources/api/#configuration","title":"Configuration","text":""},{"location":"resources/api/#environment-variables","title":"Environment Variables","text":"<p>Create <code>.env</code> file in repository root:</p> <pre><code># Ollama (local)\nOLLAMA_BASE_URL=http://localhost:11434\n\n# IBM watsonx.ai\nWATSONX_API_KEY=your_api_key\nWATSONX_PROJECT_ID=your_project_id\nWATSONX_URL=https://us-south.ml.cloud.ibm.com\n\n# OpenAI (optional)\nOPENAI_API_KEY=your_key\n\n# Anthropic (optional)\nANTHROPIC_API_KEY=your_key\n</code></pre>"},{"location":"resources/api/#audit-trail-format","title":"Audit Trail Format","text":"<p>Location: <code>traces/*.jsonl</code></p> <p>Each line is a JSON object:</p> <pre><code>{\n  \"timestamp\": \"2025-11-07T14:23:45.123Z\",\n  \"run_id\": \"experiment_001\",\n  \"model\": \"qwen2.5:7b-instruct\",\n  \"provider\": \"ollama\",\n  \"temperature\": 0.0,\n  \"seed\": 42,\n  \"task_type\": \"sql\",\n  \"prompt\": \"Generate SQL...\",\n  \"response\": \"SELECT ...\",\n  \"prompt_hash\": \"sha256:...\",\n  \"response_hash\": \"sha256:...\",\n  \"execution_time_ms\": 1245,\n  \"compliance_metrics\": {\n    \"schema_valid\": true,\n    \"citation_accuracy\": 1.0,\n    \"decision_flip\": false,\n    \"factual_drift\": 0.0\n  },\n  \"regulatory_mappings\": {\n    \"FSB\": \"consistent_decisions\",\n    \"CFTC\": \"document_ai_outcomes\",\n    \"SR_11_7\": \"model_validation\"\n  }\n}\n</code></pre>"},{"location":"resources/api/#common-patterns","title":"Common Patterns","text":""},{"location":"resources/api/#running-experiments","title":"Running Experiments","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\n# Run with deterministic settings\nresponse = client.chat.completions.create(\n    model=\"qwen2.5:7b-instruct\",\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    temperature=0.0,\n    seed=42\n)\n</code></pre>"},{"location":"resources/api/#calculating-consistency","title":"Calculating Consistency","text":"<pre><code>import json\nfrom collections import Counter\n\nwith open(\"traces/experiment.jsonl\") as f:\n    traces = [json.loads(line) for line in f]\n\nresponse_hashes = [t[\"response_hash\"] for t in traces]\nunique_count = len(set(response_hashes))\nconsistency_pct = (1 / unique_count) * 100 if unique_count &gt; 0 else 100.0\n\nprint(f\"Consistency: {consistency_pct:.1f}%\")\n</code></pre>"},{"location":"resources/api/#cross-provider-comparison","title":"Cross-Provider Comparison","text":"<pre><code>from rapidfuzz.distance import Levenshtein\n\ndistance = Levenshtein.normalized_distance(output1, output2)\nsimilarity = 1.0 - distance\n\nprint(f\"Similarity: {similarity:.1%}\")\nprint(f\"Match: {similarity &gt;= 0.95}\")\n</code></pre>"},{"location":"resources/api/#metrics-definitions","title":"Metrics Definitions","text":"Metric Formula Interpretation Consistency <code>(identical_runs / total_runs) * 100</code> % of runs producing same output Mean Drift <code>avg(Jaccard_distance(response_i, response_j))</code> Average token-level difference Similarity <code>1.0 - Levenshtein.normalized_distance(s1, s2)</code> Edit distance similarity Schema Validity JSON schema validation pass/fail Structured output compliance"},{"location":"resources/api/#further-reading","title":"Further Reading","text":"<ul> <li>Lab 2: Framework Components</li> <li>Lab 5: Cross-Provider Testing</li> <li>GitHub Repository</li> </ul>"},{"location":"resources/paper/","title":"Research Paper Summary","text":""},{"location":"resources/paper/#llm-output-drift-cross-provider-validation-mitigation-for-financial-workflows","title":"LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows","text":"<p>This page summarizes the key findings from our research paper on output drift in large language models used for financial applications.</p>"},{"location":"resources/paper/#the-core-problem","title":"The Core Problem","text":"<p>Large Language Models (LLMs) exhibit output drift: non-deterministic behavior where the same prompt produces different outputs across multiple runs, even at temperature=0.0. For financial institutions subject to regulations like SR 11-7 (Model Risk Management), ECOA, and GDPR, this creates significant compliance risks.</p> <p>The Question: Can smaller models be more reliable than larger ones for deterministic, compliance-critical tasks?</p>"},{"location":"resources/paper/#the-counterintuitive-finding","title":"The Counterintuitive Finding","text":""},{"location":"resources/paper/#smaller-models-win-for-determinism","title":"Smaller Models Win for Determinism","text":"<p>Our research reveals a counterintuitive result:</p> <ul> <li>7-8B parameter models: Achieve 100% output consistency at temperature=0.0</li> <li>120B parameter models: Only 12.5% consistency [95% CI: 3.5\u201336.0%] under identical conditions</li> </ul> <p>This challenges the conventional wisdom that \"bigger is always better\" in AI systems.</p> <p>Statistical Notation Used in This Paper</p> <p>Throughout our findings, we report:</p> <ul> <li>95% Confidence Interval (CI): The range within which we are 95% confident the true consistency rate lies. For example, \"12.5% [3.5\u201336.0]\" means the measured consistency was 12.5%, but the true value likely falls between 3.5% and 36.0%.</li> <li>\ud835\udc5d-value: Measures whether differences between models are statistically significant. Values \ud835\udc5d &lt; 0.05 indicate significance; \ud835\udc5d &lt; 0.0001 indicates highly significant differences unlikely due to chance.</li> </ul> <p>All Tier 1 vs Tier 3 comparisons showed \ud835\udc5d &lt; 0.0001, indicating the performance differences are highly statistically significant.</p>"},{"location":"resources/paper/#why-this-matters","title":"Why This Matters","text":"<p>For regulated financial applications requiring reproducible audit trails, smaller models are not just adequate\u2014they are superior to larger models when deterministic behavior is required.</p>"},{"location":"resources/paper/#methodology","title":"Methodology","text":""},{"location":"resources/paper/#experimental-design","title":"Experimental Design","text":"<ul> <li>Models Tested:</li> <li>Tier 1 (7-8B): Qwen2.5-7B, IBM Granite-3-8B</li> <li>Tier 2 (40-70B): Llama-3.3-70B, Mistral-Medium</li> <li> <p>Tier 3 (120B+): GPT-OSS-120B</p> </li> <li> <p>Total Runs: 480 experiments (n=16 concurrent runs per condition)</p> </li> <li>Tasks: SQL generation, RAG Q&amp;A, JSON summarization</li> <li>Providers: Ollama (local), IBM watsonx.ai (cloud), OpenAI, Anthropic</li> <li>Key Parameters: temperature=0.0, seed=42 (deterministic settings)</li> </ul>"},{"location":"resources/paper/#reproducibility","title":"Reproducibility","text":"<p>All experiments are reproducible using release v0.1.0:</p> <pre><code>git clone https://github.com/ibm-client-engineering/output-drift-financial-llms\ncd output-drift-financial-llms\npip install -r requirements.txt\npython run_evaluation.py \\\n  --models qwen2.5:7b-instruct,granite-3-8b,llama-3.3-70b \\\n  --temperatures 0.0,0.2 \\\n  --concurrency 1,4,16 \\\n  --repeats 16\n</code></pre>"},{"location":"resources/paper/#key-findings","title":"Key Findings","text":""},{"location":"resources/paper/#1-3-tier-model-classification","title":"1. 3-Tier Model Classification","text":"<p>Based on output consistency at temperature=0.0:</p> Tier Models Consistency Compliance Status Tier 1 7-8B (Qwen2.5-7B, Granite-3-8B) 100% \u2705 Audit-ready Tier 2 40-70B (Llama-3.3-70B, Mistral-Medium) 56-100% \u26a0\ufe0f Task-specific Tier 3 120B+ (GPT-OSS-120B) 12.5% \u274c Non-compliant <p>Interpretation: - Tier 1: Can be deployed in regulated environments requiring deterministic behavior - Tier 2: Requires careful task-specific validation - Tier 3: Unsuitable for compliance-critical applications despite superior general capabilities</p>"},{"location":"resources/paper/#2-task-specific-results-temperature00","title":"2. Task-Specific Results (Temperature=0.0)","text":"Task Type Tier 1 (7-8B) Tier 2 (40-70B) Tier 3 (120B) SQL Generation 100% 100% 12.5% Summarization 100% 87.5% 12.5% RAG Q&amp;A 93.75% 75.0% 12.5% <p>Key Insight: Even for less structured tasks (RAG), Tier 1 models maintain &gt;90% consistency.</p>"},{"location":"resources/paper/#3-temperature-sensitivity","title":"3. Temperature Sensitivity","text":"<p>RAG task consistency as temperature increases:</p> Temperature Qwen2.5-7B (Tier 1) Llama-3.3-70B (Tier 2) GPT-OSS-120B (Tier 3) T=0.0 93.75% 75.0% 12.5% T=0.2 56.25% 43.75% 6.25% T=1.0 18.75% 12.5% 0% <p>Takeaway: Even small temperature increases (0.0 \u2192 0.2) cause significant drift. For compliance, T=0.0 is mandatory.</p>"},{"location":"resources/paper/#4-cross-provider-validation","title":"4. Cross-Provider Validation","text":"<p>Testing Tier 1 model consistency across providers:</p> Provider Pair Model Consistency Validated Ollama \u2194 watsonx.ai Qwen2.5-7B \u2192 Granite-3-8B \u226595% \u2705 Ollama \u2194 watsonx.ai Granite-3-8B \u2192 Granite-3-8B 100% \u2705 Ollama \u2194 OpenAI Qwen2.5-7B \u2192 GPT-4 &lt;50% \u274c <p>Finding: Tier 1 models enable seamless migration between local (Ollama) and cloud (watsonx.ai) deployments without behavioral changes.</p>"},{"location":"resources/paper/#5-regulatory-alignment","title":"5. Regulatory Alignment","text":"<p>Our framework addresses specific regulatory requirements:</p> Regulation Requirement Framework Solution SR 11-7 Model validation &amp; ongoing monitoring Bi-temporal audit trails ECOA Consistent credit decisions 100% SQL consistency (Tier 1) FCRA Reproducible adverse action rationales Deterministic RAG retrieval GDPR Art. 22 Explainable automated decisions Citation validation FSB Third-party model risk Cross-provider validation CFTC 23.402 Predictive model documentation JSONL audit format"},{"location":"resources/paper/#technical-innovations","title":"Technical Innovations","text":""},{"location":"resources/paper/#1-deterministicretriever","title":"1. DeterministicRetriever","text":"<p>Ensures reproducible SEC 10-K retrieval with multi-key ordering:</p> <ul> <li>Problem: Standard RAG systems use non-deterministic vector similarity</li> <li>Solution: Multi-level sorting (score \u2192 document_id \u2192 chunk_id) ensures identical results</li> <li>Benefit: Same query always returns same chunks in same order</li> </ul>"},{"location":"resources/paper/#2-crossprovidervalidator","title":"2. CrossProviderValidator","text":"<p>Validates consistency across deployment environments:</p> <ul> <li>Problem: Models behave differently on different infrastructure</li> <li>Solution: Automated comparison with finance-calibrated tolerance (\u00b15% GAAP)</li> <li>Benefit: Certify migration safety before production deployment</li> </ul>"},{"location":"resources/paper/#3-bi-temporal-audit-trails","title":"3. Bi-Temporal Audit Trails","text":"<p>JSONL format capturing:</p> <ul> <li>Input prompt + response hashes (SHA-256)</li> <li>Model parameters (temperature, seed, version)</li> <li>Compliance metrics (schema validity, citation accuracy)</li> <li>Regulatory mappings (SR 11-7, ECOA, FCRA)</li> </ul>"},{"location":"resources/paper/#practical-implications","title":"Practical Implications","text":""},{"location":"resources/paper/#for-financial-institutions","title":"For Financial Institutions","text":"<ol> <li>Vendor Selection: Prioritize Tier 1 models (7-8B) for compliance-critical tasks</li> <li>Temperature Policy: Mandate T=0.0 for all regulated applications</li> <li>Model Validation: Use cross-provider validation before production deployment</li> <li>Audit Trail: Implement bi-temporal logging per CFTC 23.402 requirements</li> </ol>"},{"location":"resources/paper/#for-model-developers","title":"For Model Developers","text":"<ol> <li>Architecture: Optimize for determinism, not just accuracy</li> <li>Testing: Include multi-run consistency metrics in benchmarks</li> <li>Documentation: Report consistency scores alongside performance metrics</li> </ol>"},{"location":"resources/paper/#for-regulators","title":"For Regulators","text":"<ol> <li>Standards: Define acceptable consistency thresholds (our research suggests 100% for Tier 1 tasks)</li> <li>Validation: Require cross-provider equivalence testing</li> <li>Monitoring: Mandate ongoing drift detection in production</li> </ol>"},{"location":"resources/paper/#limitations","title":"Limitations","text":"<ol> <li>Model Scope: Tested 5 models; findings may not generalize to all architectures</li> <li>Task Coverage: Focused on SQL, RAG, summarization\u2014other tasks (e.g., generation) may differ</li> <li>Infrastructure: Results specific to tested providers (Ollama, watsonx.ai)</li> <li>Temporal Stability: Long-term consistency (months/years) not evaluated</li> </ol>"},{"location":"resources/paper/#future-work","title":"Future Work","text":"<ol> <li>Expanded Model Coverage: Test emerging architectures (Gemma-2-9B, Phi-4, etc.)</li> <li>Additional Tasks: Credit risk, fraud detection, portfolio optimization</li> <li>Regulatory Integration: Pilot with partner banks under SR 11-7 supervision</li> <li>Drift Mitigation: Techniques to improve Tier \u2154 consistency</li> </ol>"},{"location":"resources/paper/#citation","title":"Citation","text":"<p>If you use this framework or findings in your research, please cite:</p> <pre><code>@article{khatchadourian2025output,\n  title={LLM Output Drift: Financial AI Compliance Framework},\n  author={Khatchadourian, Raffi and Franco, Rolando},\n  journal={arXiv preprint arXiv:2511.07585},\n  year={2025}\n}\n</code></pre> <p>Paper: arXiv:2511.07585 | DOI: 10.48550/arXiv.2511.07585</p>"},{"location":"resources/paper/#related-resources","title":"Related Resources","text":"<ul> <li>Full Paper: arXiv:2511.07585</li> <li>Code Repository: GitHub</li> <li>API Documentation: API Reference</li> <li>Workshop Labs: Lab 0-6</li> </ul>"},{"location":"resources/paper/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Size isn't everything: 7-8B models outperform 120B models for deterministic tasks</li> <li>Temperature=0.0 is mandatory: Even T=0.2 causes significant drift</li> <li>Tier 1 models are audit-ready: 100% consistency enables regulatory compliance</li> <li>Cross-provider validation works: Seamless migration between Ollama and watsonx.ai</li> <li>Framework is open source: MIT-licensed, production-ready, extensible</li> </ol> <p>Questions? See Troubleshooting Guide or open an issue on GitHub.</p>"},{"location":"resources/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Common issues and solutions for the Output Drift framework.</p>"},{"location":"resources/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"resources/troubleshooting/#python-version-mismatch","title":"Python Version Mismatch","text":"<p>Problem: ImportError or syntax errors</p> <p>Solution: <pre><code># Check Python version (must be 3.11+)\npython --version\n\n# Use specific version if multiple installed\npython3.11 -m venv venv\nsource venv/bin/activate\n</code></pre></p>"},{"location":"resources/troubleshooting/#dependency-conflicts","title":"Dependency Conflicts","text":"<p>Problem: Package version conflicts</p> <p>Solution: <pre><code># Fresh virtual environment\nrm -rf venv\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install -r requirements.txt\n</code></pre></p>"},{"location":"resources/troubleshooting/#ollama-issues","title":"Ollama Issues","text":""},{"location":"resources/troubleshooting/#connection-refused","title":"Connection Refused","text":"<p>Problem: <code>ConnectionRefusedError: [Errno 61] Connection refused</code></p> <p>Solution: <pre><code># Check if Ollama is running\ncurl http://localhost:11434/api/tags\n\n# If not running, start it\nollama serve\n\n# Pull model if missing\nollama pull qwen2.5:7b-instruct\n</code></pre></p>"},{"location":"resources/troubleshooting/#model-not-found","title":"Model Not Found","text":"<p>Problem: <code>Model 'qwen2.5:7b-instruct' not found</code></p> <p>Solution: <pre><code># List available models\nollama list\n\n# Pull the model\nollama pull qwen2.5:7b-instruct\n\n# Verify\nollama list | grep qwen\n</code></pre></p>"},{"location":"resources/troubleshooting/#slow-performance","title":"Slow Performance","text":"<p>Problem: Ollama responses taking &gt;10 seconds</p> <p>Solution: <pre><code># Check system resources\ntop\n\n# Ensure GPU is being used (if available)\nollama show qwen2.5:7b-instruct | grep parameters\n\n# Reduce concurrency if CPU-only\n--concurrency 4  # instead of 16\n</code></pre></p>"},{"location":"resources/troubleshooting/#watsonxai-issues","title":"watsonx.ai Issues","text":""},{"location":"resources/troubleshooting/#authentication-failed","title":"Authentication Failed","text":"<p>Problem: <code>401 Unauthorized</code> or <code>Invalid API key</code></p> <p>Solution: <pre><code># Test credentials\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nprint(f\"API Key: {os.getenv('WATSONX_API_KEY')[:10]}...\")\nprint(f\"Project ID: {os.getenv('WATSONX_PROJECT_ID')}\")\nprint(f\"URL: {os.getenv('WATSONX_URL')}\")\n\n# Verify all are set\nassert all([os.getenv('WATSONX_API_KEY'),\n            os.getenv('WATSONX_PROJECT_ID'),\n            os.getenv('WATSONX_URL')]), \"Missing credentials\"\n</code></pre></p> <p>Check: 1. <code>.env</code> file exists in repository root 2. No extra spaces or quotes in <code>.env</code> 3. API key has not expired 4. Project ID is correct</p>"},{"location":"resources/troubleshooting/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: <code>429 Too Many Requests</code></p> <p>Solution: <pre><code># Add delay between requests\nimport time\n\nfor i in range(16):\n    response = model.generate_text(prompt, params)\n    time.sleep(1)  # 1 second delay\n</code></pre></p> <p>Or use built-in rate limiting: <pre><code>python run_evaluation.py \\\n  --rate-limit 10 \\  # Max 10 requests/minute\n  --retry-delay 5    # 5 seconds between retries\n</code></pre></p>"},{"location":"resources/troubleshooting/#database-issues","title":"Database Issues","text":""},{"location":"resources/troubleshooting/#sqlite-not-found","title":"SQLite Not Found","text":"<p>Problem: <code>sqlite3.OperationalError: no such table: transactions</code></p> <p>Solution: <pre><code># Regenerate database\npython data/generate_toy_finance.py\n\n# Verify tables\nsqlite3 data/toy_finance.sqlite \"SELECT name FROM sqlite_master WHERE type='table';\"\n</code></pre></p>"},{"location":"resources/troubleshooting/#schema-mismatch","title":"Schema Mismatch","text":"<p>Problem: SQL queries fail with schema errors</p> <p>Solution: <pre><code># Check schema\nsqlite3 data/toy_finance.sqlite \".schema transactions\"\n\n# Expected output:\n# CREATE TABLE transactions(\n#   id INTEGER PRIMARY KEY,\n#   date TEXT,\n#   region TEXT,\n#   amount REAL,\n#   category TEXT\n# );\n</code></pre></p>"},{"location":"resources/troubleshooting/#drift-detection-issues","title":"Drift Detection Issues","text":""},{"location":"resources/troubleshooting/#false-positives-detecting-drift-when-there-isnt-any","title":"False Positives (Detecting Drift When There Isn't Any)","text":"<p>Problem: Tier 1 models showing &lt;100% consistency</p> <p>Causes: 1. Non-deterministic seed: Ensure <code>seed=42</code> is set 2. Different model versions: Check <code>ollama show model</code> 3. System prompt variations: Use exact prompts from templates 4. Whitespace differences: Normalize before comparison</p> <p>Solution: <pre><code># Normalize outputs before comparison\nimport re\n\ndef normalize(text: str) -&gt; str:\n    \"\"\"Remove extra whitespace and normalize case.\"\"\"\n    return re.sub(r'\\s+', ' ', text.strip().lower())\n\noutput1_norm = normalize(output1)\noutput2_norm = normalize(output2)\nmatch = (output1_norm == output2_norm)\n</code></pre></p>"},{"location":"resources/troubleshooting/#false-negatives-not-detecting-real-drift","title":"False Negatives (Not Detecting Real Drift)","text":"<p>Problem: Drift exists but isn't detected</p> <p>Causes: 1. n too small: Must use n\u226516 for reliable detection 2. Hash collisions: Unlikely but possible with SHA-256 3. Semantic drift not captured: Same tokens, different meaning</p> <p>Solution: <pre><code># Always use n=16 (as in paper)\n--concurrency 16\n\n# Additionally check factual consistency (for RAG)\n--validate-facts true\n</code></pre></p>"},{"location":"resources/troubleshooting/#api-errors","title":"API Errors","text":""},{"location":"resources/troubleshooting/#openai-client-issues","title":"OpenAI Client Issues","text":"<p>Problem: <code>openai.APIError</code> or similar</p> <p>Solution: <pre><code># For Ollama, use OpenAI client with custom base_url\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:11434/v1\",  # Note: /v1 suffix\n    api_key=\"ollama\"  # Ollama doesn't check API key\n)\n</code></pre></p>"},{"location":"resources/troubleshooting/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'harness'</code></p> <p>Solution: <pre><code># Ensure you're in the repository root\ncd /path/to/output-drift-financial-llms\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Verify installation\npython -c \"import harness; print('\u2705 Import successful')\"\n</code></pre></p>"},{"location":"resources/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"resources/troubleshooting/#memory-errors","title":"Memory Errors","text":"<p>Problem: <code>MemoryError</code> or system freezing</p> <p>Solution: <pre><code># Reduce concurrency\n--concurrency 4  # Instead of 16\n\n# Process in batches\n--batch-size 4\n\n# Use smaller model\nqwen2.5:7b-instruct  # Instead of larger models\n</code></pre></p>"},{"location":"resources/troubleshooting/#slow-experiments","title":"Slow Experiments","text":"<p>Problem: Experiments taking &gt;1 hour</p> <p>Solution: <pre><code># Profile time per request\ntime curl -X POST http://localhost:11434/api/generate \\\n  -d '{\"model\": \"qwen2.5:7b-instruct\", \"prompt\": \"test\"}'\n\n# If &gt;5 seconds per request:\n# 1. Check CPU/GPU utilization\n# 2. Reduce model size\n# 3. Increase system resources\n</code></pre></p>"},{"location":"resources/troubleshooting/#cross-provider-issues","title":"Cross-Provider Issues","text":""},{"location":"resources/troubleshooting/#outputs-dont-match","title":"Outputs Don't Match","text":"<p>Problem: Ollama and watsonx outputs differ significantly</p> <p>Expected: Tier 1 \u2192 Tier 1 should match \u226595%</p> <p>Debugging: <pre><code># Compare raw outputs\nprint(\"Ollama:\", repr(ollama_output))\nprint(\"watsonx:\", repr(watsonx_output))\n\n# Check lengths\nprint(f\"Ollama length: {len(ollama_output)}\")\nprint(f\"watsonx length: {len(watsonx_output)}\")\n\n# Calculate similarity\nfrom rapidfuzz.distance import Levenshtein\nsim = 1.0 - Levenshtein.normalized_distance(ollama_output, watsonx_output)\nprint(f\"Similarity: {sim:.1%}\")\n</code></pre></p> <p>Common causes: 1. Different model versions (Qwen vs Granite) 2. Temperature not exactly 0.0 3. System prompts differ 4. Different tokenization</p>"},{"location":"resources/troubleshooting/#compliance-validation-errors","title":"Compliance Validation Errors","text":""},{"location":"resources/troubleshooting/#schema-validation-fails","title":"Schema Validation Fails","text":"<p>Problem: <code>jsonschema.ValidationError</code></p> <p>Solution: <pre><code>import json\nfrom jsonschema import validate, ValidationError\n\n# Test JSON parsing\ntry:\n    data = json.loads(response)\n    print(\"\u2705 Valid JSON\")\nexcept json.JSONDecodeError as e:\n    print(f\"\u274c Invalid JSON: {e}\")\n\n# Test schema validation\ntry:\n    validate(data, schema)\n    print(\"\u2705 Schema valid\")\nexcept ValidationError as e:\n    print(f\"\u274c Schema invalid: {e.message}\")\n</code></pre></p>"},{"location":"resources/troubleshooting/#common-error-messages","title":"Common Error Messages","text":""},{"location":"resources/troubleshooting/#filenotfounderror-errno-2-no-such-file-or-directory-traces","title":"<code>FileNotFoundError: [Errno 2] No such file or directory: 'traces/'</code>","text":"<p>Solution: <pre><code>mkdir -p traces\n</code></pre></p>"},{"location":"resources/troubleshooting/#runtimeerror-found-no-nvidia-driver-on-your-system","title":"<code>RuntimeError: Found no NVIDIA driver on your system</code>","text":"<p>Not an error - Ollama will use CPU, which is fine for 7-8B models.</p>"},{"location":"resources/troubleshooting/#importerror-cannot-import-name-deterministicretriever","title":"<code>ImportError: cannot import name 'DeterministicRetriever'</code>","text":"<p>Solution: <pre><code># Ensure harness/__init__.py exists\nls harness/__init__.py\n\n# Reinstall if missing\npip install -e .\n</code></pre></p>"},{"location":"resources/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If you're still stuck:</p> <ol> <li> <p>Check Logs:    <pre><code># Ollama logs\nollama logs\n\n# Python logs\npython run_evaluation.py --verbose\n</code></pre></p> </li> <li> <p>GitHub Issues: Open an issue</p> </li> <li> <p>Email Support: Contact maintainers (see README)</p> </li> <li> <p>Review Documentation:</p> </li> <li>API Reference</li> <li>Lab Guides</li> <li>Research Paper</li> </ol>"},{"location":"resources/troubleshooting/#quick-diagnostics-script","title":"Quick Diagnostics Script","text":"<p>Run this to check your environment:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Quick diagnostics for Output Drift framework.\"\"\"\nimport sys\nimport os\nimport subprocess\n\nprint(\"\ud83d\udd0d Diagnostics\\n\" + \"=\" * 60)\n\n# Python version\nprint(f\"Python: {sys.version}\")\nassert sys.version_info &gt;= (3, 11), \"\u274c Python 3.11+ required\"\nprint(\"\u2705 Python version OK\\n\")\n\n# Dependencies\ntry:\n    import openai, pandas, matplotlib\n    print(\"\u2705 Dependencies installed\\n\")\nexcept ImportError as e:\n    print(f\"\u274c Missing dependency: {e}\\n\")\n\n# Ollama\ntry:\n    result = subprocess.run([\"curl\", \"-s\", \"http://localhost:11434/api/tags\"],\n                          capture_output=True, timeout=5)\n    if result.returncode == 0:\n        print(\"\u2705 Ollama running\\n\")\n    else:\n        print(\"\u26a0\ufe0f  Ollama not responding\\n\")\nexcept Exception as e:\n    print(f\"\u274c Ollama check failed: {e}\\n\")\n\n# Environment variables\nenv_vars = [\"WATSONX_API_KEY\", \"WATSONX_PROJECT_ID\"]\nfor var in env_vars:\n    if os.getenv(var):\n        print(f\"\u2705 {var} set\")\n    else:\n        print(f\"\u26a0\ufe0f  {var} not set\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Diagnostics complete!\")\n</code></pre> <p>Save as <code>diagnostics.py</code> and run: <pre><code>python diagnostics.py\n</code></pre></p>"}]}